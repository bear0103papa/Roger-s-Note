---
layout: post
title: "Anthropic 執行長 Dario Amodei 談如何在人工智慧末日中生存"
date: 2025-03-03
categories: [
        "Dario Amodei", 
        "Kevin Roose",
        "Casey Newton",
        "人工智能",
        "Anthropic",
        "Claude",
        "2025"
]
description: "這篇文章主要介紹了Anthropic公司的Claude 3.7 Sonnet模型，重點討論了其推理能力的提升、與競爭對手的差異，以及AI安全風險評估。Dario Amodei解釋了Claude 3.7如何專注於現實世界任務而非僅限於數學和競賽編程，並討論了AI發展的全球競爭格局，特別是與中國的技術競賽。文章還探討了AI安全問題、未來潛在風險，以及AI如何在醫療診斷等領域帶來積極變革。"
toc: true  # 啟用目錄功能

---

<span class="original-link">原文連結： [Anthropic 執行長 Dario Amodei 談如何在人工智慧末日中生存](https://www.youtube.com/watch?v=YhGUSIvsn_Y&ab_channel=HardFork)</span>

## Claude 3.7 Sonnet：推理與思考的新境界

**Kevin Roose**: 那麼，告訴我們有關 Claude 3.7 的信息吧，這個新模型是怎麼樣的？

**Dario Amodei**: 是的，我們已經在這個模型上工作了一段時間。基本上，我們心中有兩個主要目標。首先，當然，市場上已有一些推理模型，我們也想製作我們自己的模型。但我們希望將重點放在不同的地方。

具體來說，市場上的許多推理模型主要訓練於數學和競賽編程，這些是可以衡量性能的客觀任務。我不是說它們不令人印象深刻，但它們有時對現實世界或經濟中的任務來說並不那麼相關。即使是在編程領域，競賽編程和在現實世界中進行編程也有很大的區別。因此，我們訓練 Claude 3.7 更加專注於這些現實世界的任務。

我們也覺得有點奇怪的是，在一些推理模型中，通常會有一個常規模型，然後有一個推理模型。這就像如果一個人擁有兩個大腦一樣。如果你問我一個簡單的問題，比如你的名字是什麼，那麼你是在和大腦一號對話。如果你問我如何證明一個數學定理，那麼你就得和大腦二號對話，因為我需要坐下來思考 20 分鐘。

**Kevin Roose**: 是的。這就像一個播客，有兩個主持人，其中一個人總是喜歡喋喋不休，而另一個則在說話前會先思考。

**Casey Newton**: 哦，拜託。

**Kevin Roose**: [笑]

**Casey Newton**: 太殘酷了。

**Dario Amodei**: 不發表評論。

**Casey Newton**: 太殘酷了。

**Dario Amodei**: 不發表評論，與任何相關性無關。

**Kevin Roose**: 那麼，與之前的模型相比，Claude 3.7 的用戶會注意到哪些差異呢？

**Dario Amodei**: 是的，幾點來說，這個模型會更好，包括在編程方面更出色，Claude 模型一直是最擅長編程的，而 3.7 又更進了一步。除了模型本身的特性，你還可以將它設置為這種擴展思考模式，在這種模式下，你基本上告訴它——實際上還是同一個模型，只是你告訴它以一種可以長時間思考的方式運行。如果你是 API 用戶，你甚至可以告訴它，你可以思考的時間邊界。

**Casey Newton**: 只是澄清一下，因為這可能會讓一些人感到困惑，你的意思是新的 Claude 是這個混合模型。它有時可以進行推理，有時可以給出較快的回答。但如果你希望它思考更久，那是另一個模式。

**Dario Amodei**: 那是另一個模式。

**Casey Newton**: 思考和推理有點像是分開的模式。

**Dario Amodei**: 是的，是的。基本上，模型可以像平常一樣回答問題，或者你可以給它一個指示，讓它思考更長時間。更進一步的發展方向是，模型會自行決定適合思考的時間。人類就是這樣，或者至少可以這樣。

如果我問你你的名字，你不會想，嗯，我應該想多久才能回答？給我 20 分鐘來確定我的名字。但如果我說，嘿，我想讓你分析這隻股票，或者我想讓你證明這個數學定理，能夠做這些任務的人類不會立刻給出答案。他們會說，好吧，這需要一段時間，然後會記下任務，再去得出答案。

**Kevin Roose**: 這是我對當今語言模型和 AI 模型的一個主要抱怨，就是我會使用像是 ChatGPT 這樣的東西，然後忘記自己處於硬核推理模式。我會問它一些愚蠢的問題，比如「如何更改熱水器的設置」，然後它會思考四分鐘。我會想，實際上我並不想這樣。

**Dario Amodei**: 它可能會給你寫一篇有關調整熱水器溫度的論文。

**Kevin Roose**: 熱水器的歷史。

**Dario Amodei**: 你知道的，第一個考量。

**Kevin Roose**: 那你覺得多久之後，模型能夠自動處理這種路由功能呢？也就是說，你問它一個問題，它會根據問題的複雜程度決定需要多長的思考時間。比如對這個問題，你可能需要 3 分鐘的思考時間，而對另一個問題，可能只需要 30 秒。

**Dario Amodei**: 是的。我覺得我們的模型在朝這個方向邁進。即使是在 API 中，如果你給它設置思考的邊界——比如說，我會想 20,000 字或之類的——通常來說，即使給它最多 20,000 字，它大部分時間也不會使用到那麼多。某些時候，它甚至會給出非常簡短的回答。因為當它知道進一步思考不會帶來更多的收穫時，它就不會延長思考時間。但是，給出思考時間的邊界還是很有價值的。所以我們在這方面已經邁出了很大的一步，但還沒有達到我們希望的狀態。

**Casey Newton**: 當你說它在現實世界的任務上更好時，你想到的是哪些任務？

**Dario Amodei**: 是的。我認為，最重要的是編程。Claude 模型在現實世界的編程中表現得非常出色。我們有許多客戶，從 Cursor 到 GitHub，再到 Windsurf Codeium、Cognition、Vercel，還有——我肯定還漏掉了一些，不過——

**Kevin Roose**: 這些是與編程相關的應用。

**Casey Newton**: 或者說，僅僅是編程應用。

**Kevin Roose**: 是的。

**Dario Amodei**: 編程應用，確實有許多不同種類的編程應用。我們還發布了叫做 Claude Code 的東西，這是一個更像命令行的工具。但我認為，像是複雜指令的跟隨，或者說是這樣的：我想讓你理解這份文件，或者我希望你使用這一系列工具，我們訓練的推理模型 Claude 3.7 Sonnet 在這些任務上也表現得更好。

**Casey Newton**: 是的。新版本的 Claude Sonnet 有一件事是沒做的，Dario，就是它不能訪問互聯網。

**Dario Amodei**: 是的。

**Casey Newton**: 為什麼不呢？什麼情況下會讓你們改變這一點？

**Dario Amodei**: 是的。我記得之前說過這個，但網絡搜索功能很快就會推出。

**Casey Newton**: 好的。

**Dario Amodei**: 我們很快就會有網絡搜索功能。我們承認這是一個疏忽。我覺得總的來說，我們往往更專注於企業市場而非消費者市場，這其實更像是消費者功能，雖然它也可以被企業使用。但我們關注兩者，這個功能快要來了。

**Casey Newton**: 明白了。所以你們給這個模型命名為 3.7，上一個模型是 3.5。去年你們悄悄更新了它，業界內部把那個稱為 3.6。說實話，這讓我們所有人都快抓狂了。AI 模型命名到底怎麼回事？

**Dario Amodei**: 我們是最不瘋狂的，雖然我承認我們也有點瘋狂。所以，聽著，我認為我們的錯誤是相對可以理解的。我們做了 3.5 Sonnet，做得很好，然後有了三個 3.0 和 3.5。我知道 3.7 新版本是一個錯誤步驟。事實證明，改變 API 中的名稱其實很難，特別是當你有這麼多合作夥伴和提供的服務時。

**Kevin Roose**: 你們應該能搞定的，我相信你們。

**Dario Amodei**: 不，不，不。這比訓練模型還難，我告訴你。

所以我們已經事後並正式地把上一個版本命名為 3.6，這樣 3.7 就顯得合理了。我們也將保留 Claude 4 Sonnet，可能還有一些後續的模型，用於真正具有重大飛躍的版本。

**Casey Newton**: 有時候當模型 —

**Dario Amodei**: 那些模型正在來，順便說一下。

**Casey Newton**: 好的。

**Kevin Roose**: 明白了。什麼時候來？

**Dario Amodei**: 是的，對，我應該談談這個。到目前為止，我們發布的所有模型實際上並不那麼昂貴，對吧？我寫過一篇博文說過，它們的成本最多在幾千萬美元範圍內。確實有更大的模型，它們正在來。這些模型需要很長時間，有時需要很長時間才能做好。但是這些更大的模型，無論是我們還是競爭對手，都會推出。我們離推出一個更大的基礎模型不遠了。

所以，Claude 3.7 Sonnet 和 Claude 3.6 Sonnet 的大部分改進，其實是在訓練後的階段。但我們也在開發更強大的基礎模型，或許這會是 Claude 4 系列，也許不是。我們會看看。但我認為它們在相對較短的時間內會推出。

**Casey Newton**: 一段較短的時間。我會把這個記在日曆上。幾個時間單位後提醒我檢查一下，Kevin。

## AI安全與風險評估：從中等風險到潛在危機

**Kevin Roose**: 我知道你們 Anthropic 團隊非常關注 AI 安全以及你們推出的模型的安全性。我知道你們花了很多時間來思考這些問題，並且會內部進行紅隊測試。那麼，Claude 3.7 Sonnet 是否有任何新功能是危險的，或者可能會讓關心 AI 安全的人擔憂的呢？

**Dario Amodei**: 所以並不會說是“危險”本身。我總是想要澄清這一點，因為我覺得現在有一種常見的誤解，就是把當前的風險和未來的風險混淆了。不是說當前沒有風險，當然有，而且總有一些正常的技術風險和技術政策問題。我更擔心的是，當模型變得更強大時，我們可能會看到的風險。

我認為這些風險——在2023年我們討論過很多，我記得我還曾在參議院作證，談到過諸如濫用風險，比如生物或化學戰爭的風險，或者AI自主風險。特別是對於濫用風險，我當時說過，我不知道這些風險何時會成為現實，但可能會在2025年或2026年出現。

現在，到了2025年的初期，我認為模型開始接近那些風險，尤其是在Claude 3.7 Sonnet中，正如我們在模型卡片中寫的那樣，我們總是進行這些——你幾乎可以稱之為試驗的實驗，就像是控制組的實驗，其中我們會找一些對某些領域（比如生物學）不太了解的人，然後看模型在幫助他們進行一些模擬不良工作流程方面能發揮多大作用。

我們會改變一些步驟，進行模擬不良工作流程的實驗。這樣的試驗中，模型能幫助這些人達到什麼樣的效果？有時我們甚至會在現實中進行濕實驗，比較他們在使用像Google這樣的工具，或者在沒有幫助的情況下能做些什麼，與他們在模型的幫助下能做到的比較。

**Kevin Roose**: 就是說在 Google 上？

**Dario Amodei**: 是的，或使用課本，或者完全不受幫助的情況下。我們想要了解的是，這是否會使某些新的風險變得更加可能，那些風險在以前並不存在。我認為非常重要的一點是，我們並不是在擔心模型會給我提供一個如何製作某物的食譜。比如“製作冰毒的配方”，這種事情很簡單，Google也可以做到。我們完全不在乎這些。

我們關心的是一些比較深奧的、高度專業的知識，像是只有病毒學博士才懂的知識。它能幫助多少？如果它能幫助，那並不意味著明天我們就會死於瘟疫，而是意味著一種新的風險出現了。這是一種新的威脅向量，類似於你剛剛讓製造核武器變得更加容易。你發明了某些東西，讓製造核武器所需的鈽量比之前少了。

因此，我們對Sonnet 3.7進行了這些風險評估，模型在這些方面變得更好了。它們還沒有達到我們認為會對結束整個過程，並完成真正危險任務的風險有實質性影響的階段。

然而，我們在模型卡片中說過，我們評估了下一個模型或未來三到六個月內某個模型，出現這些風險的概率是相當大的。我們的安全程序，負責的擴展程序，專注於這些非常大的風險，會隨之啟動，並且我們將會有額外的安全措施和額外的部署措施，特別是針對這些狹隘的風險。

**Casey Newton**: 是的，我想強調一下，你的意思是，在接下來的三到六個月內，這些模型的風險處於中等水平。顯然，如果你們處於這個階段，你們的競爭對手也可能處於同樣的情況。這實際上意味著什麼？如果我們都將生活在中等風險的環境中，世界需要做些什麼？

**Dario Amodei**: 我認為，至少在目前這個階段，這不會對事情帶來太大改變。這意味著，如果這些風險沒有得到緩解，模型能夠做到的某些事情會增加真正危險或非常不好的事情發生的風險。想像一下你是一名執法官員，或者FBI的成員。有了一個新的威脅向量，這是新型的攻擊方式。

這並不意味著世界末日，但確實意味著任何涉及這些風險的行業應該對這些風險採取特別的預防措施。

**Casey Newton**: 明白了。

**Dario Amodei**: 所以我不知道。我可能錯了，這可能會需要更長時間。你無法預測會發生什麼。但我認為，與今天的環境相比，我們不再那麼關注風險，背景中的風險實際上是在增加的。

**Casey Newton**: 我們有更多的安全問題要問，但我想首先問兩個關於創新競爭的問題。

**Dario Amodei**: 好的。

**Casey Newton**: 現在看起來，不管某家公司模型有多創新，這些創新都會在幾個月甚至幾週內被對手複製。這讓你的工作更難嗎？你覺得這會永遠如此嗎？

**Dario Amodei**: 我不認為創新必然會被完全複製。我想說的是，許多競爭者的創新速度是非常快的。有四五家公司，可能是六家公司，在非常快的速度下進行創新並快速推出模型。但如果你看看Sonnet 3.7，我們在推理模型的處理方式上，與競爭對手有所不同。我們強調的方面也有所不同。

甚至在此之前，Sonnet 3.5擅長的事情與其他模型擅長的也不一樣。人們常說競爭、商品化、成本下降，但實際上，這些模型之間是相對不同的，這就創造了區別。

**Kevin Roose**: 是的。我們收到很多來自聽眾的問題，問如果我要訂閱一個AI工具，那應該選擇哪一個？他們會說，他們用它來做這些事。我很難回答這個問題，因為我發現對於大多數使用案例來說，這些模型在回答問題方面都做得相對不錯。最終還是取決於像是你喜歡哪個模型的個性。你覺得人們會基於能力選擇AI模型嗎？還是更多是關於個性和它如何與他們互動，如何讓他們感覺？

**Dario Amodei**: 我覺得這取決於你說的是哪一類消費者。即使在消費者中，也有一些人會用這些模型來完成某些有點複雜的任務。有些人可能是獨立的，用來分析數據，這可能算是專業消費者的一部分。我認為，在這部分市場內，能力的差異是很重要的。這些模型在幫助你完成任何與生產力相關的事情或即使是計劃旅行這類複雜任務方面，能做得比現在好得多。

即便在那之外，如果你只是想製作一個個人助理來管理你的生活，或者類似的東西，我們距離能做到那樣的模型還很遠，從一個能理解你生活的各個方面並能全面給予建議的模型還有很長的路要走。我認為這之間有差異。我最好的助理可能不是別人的最佳選擇。

我認為，模型的確足夠好，可以作為Google搜索的替代品，或者作為快速的信息檢索工具使用，這就是大眾市場免費用戶的情況。我覺得這個是可以商品化的。這些模型已經基本做到這點，並且正擴展到全世界，但我不認為這是模型最有趣的使用方式。我其實不確定這其中會有多少經濟價值。

**Casey Newton**: 我的意思是，我聽到的意思是，如果你們開發出一個，假設說，一個非常出色的個人助理，那麼首先能做到的公司將會擁有很大的優勢，因為其他實驗室將更難複製它？他們怎麼復制會變得不那麼明顯。

**Dario Amodei**: 他們會更難複製它。即使他們能夠複製，它們也不會完全一樣。他們會以自己的方式復制，按照他們自己的風格，並且會適合不同的群體。所以我想我在說的是，市場其實比你想像的更細分。它看起來像是一個大市場，但實際上它比你想的要更細分。

## 全球AI競爭與未來展望：與中國的技術競賽

**Casey Newton**: 明白了。那麼讓我問一下有關競爭的問題，這也把我們帶到了安全問題。你最近寫了一篇非常有趣的文章，關於DeepSeek，這篇文章發表的時候DeepSeek正處於風頭最勁的時期。你在文中提到，部分原因是他們已經找到的成本降低方法，基本上與——這些方法基本上與成本下降的趨勢一致。但你同時也表示，DeepSeek應該是一個警鐘，因為它顯示出中國在保持與前沿實驗室同步方面，做到了之前未曾做到的事。所以，對你來說，這有何特別之處？我們應該如何應對這一點？

**Dario Amodei**: 嗯，我覺得這不完全是關於商業競爭。我比較擔心DeepSeek從國家競爭和國家安全的角度來看。我關心的點是，當我們看待世界局勢時，我們會發現像中國和俄羅斯這樣的極權國家。我其實已經擔心有十年了，擔心人工智能會成為極權主義的引擎。

如果你想想那些壓迫性政府，它們的壓迫能力通常是由它們能夠讓人類執行者做的事來決定的。但是，如果他們的執行者不再是人類，那麼就會出現一些非常黑暗的可能性。因此，我對這個領域非常擔心，我希望確保自由民主國家擁有足夠的優勢和技術優勢，以防止這些濫用行為的發生，同時也防止我們的對手在全球範圍內把我們置於不利地位，甚至威脅到我們的安全。

有一個有點奇怪且尷尬的事實是，正是美國的公司在開發這些技術，正是中國的公司也在開發這些技術。但我們不應該天真。無論這些公司的意圖如何，特別是在中國，這背後都有政府層面的影響。因此，我對確保極權國家不會在軍事上取得優勢感興趣。

我並不是想否定他們獲得這項技術的好處。其實，我希望確保這些技術的巨大健康利益能夠傳遍世界每個地方，包括最貧困的地區，包括那些處於極權政府控制下的地方。但我不希望極權政府在軍事上占據優勢。因此，像出口控制這樣的措施，我在那篇文章中也討論過，這是我們可以做的一個防範措施。我感到高興的是，特朗普政府實際上正在考慮加強出口控制。

**Kevin Roose**: 我上週末參加了一個人工智能安全會議，其中一些人對Anthropic，可能也對你本人提出了一些批評，他們認為像你寫的關於DeepSeek的文章，實際上是在推動與中國的人工智能軍備競賽，強調美國必須率先達到強大的AGI，否則就會怎樣。他們擔心這樣會加速競賽過程，可能會因此在過程中有所妥協，並且擔心加速這場競賽本身會帶來一些風險。你怎麼回應這些看法？

**Dario Amodei**: 嗯，我的看法有些不同。我認為，如果我們想要有任何機會——所以自然狀態下的情況是事情以最快速度進行。如果我們希望能夠減慢這個速度，我的計劃是這樣的。

在美國或其他民主國家內部——這些國家都在法治之下，可以通過立法來約束。我們可以讓公司和政府達成可執行的協議，或者讓公司做出可以執行的安全承諾。因此，如果我們處於一個這樣的世界，存在不同的公司，它們在自然狀態下會盡可能地加速進行，通過一些自願承諾和法律的混合，我們可以讓自己在模型過於危險時減速。

這實際上是可執行的。如果你對每個人都指著槍，你可以讓大家合作，這就是最終法律的作用。但我認為，在國際競爭的世界中，這一切就會被拋到窗外。美國和中國之間沒有任何人擁有權威來強制執行協議，即使達成了協議。

因此，我的擔心是，如果美國領先中國幾年，我們可以利用這幾年來讓事情變得更安全。如果我們與中國持平，那麼就不是推動軍備競賽了，這將是不可避免的事。這項技術具有巨大的軍事價值。無論人們現在說些什麼，無論他們說什麼關於合作的美好話語，我真的看不出一旦大家完全理解這項技術的經濟和軍事價值——我認為他們大多數人已經理解了——我看不出會有任何方式能讓它變成其他形式，除非是最激烈的競賽。

因此，我能想到的方式是，如果我們能夠減緩專制國家的步伐，幾乎可以消除這一權衡，讓我們有更多時間來研究如何確保這些模型的安全，無論是OpenAI、Google還是X.AI。

現在，某一時點我們是否能說服專制國家——例如說服中國，讓他們認識到這些模型的危險性，並達成某種協議，並想出一些方法來強制執行？我其實也支持嘗試這樣做，但這不應該是計劃A。這不是一種現實的世界觀。

**Casey Newton**: 這些似乎是非常重要的問題和討論，然而看來在你和Kevin幾週前參加的巴黎AI行動峰會上，大多數這樣的討論並未進行。那麼那個峰會到底發生了什麼事情？

**Dario Amodei**: 是的，我得說，我對那次峰會感到非常失望。它有點像一個貿易展的氛圍，完全和當初英國政府在布萊奇利公園創辦的峰會精神背道而馳。布萊奇利做得很好，英國政府做得也很好，他們在不知道自己在做什麼之前並沒有引入一堆繁重的法規，但他們說，嘿，我們召集這些峰會來討論風險。

我認為那是很好的做法。我覺得現在這種做法已經消失了，這可能是總體上不再那麼關注風險，更加渴望抓住機會的趨勢的一部分。我當然支持抓住機會，對吧？我寫過那篇《Machines of Loving Grace》，其中提到很多美好的事物。那篇文章的一部分內容就是，對於一個擔心風險的人來說，我感覺自己對這些技術的好處的視角，比起那些只談論好處的人要更清晰。

但是在背景中，正如我所說，隨著模型變得越來越強大，我們能做的驚人而美好的事情的數量也增加了——但風險也在增加。這種逐步增加的風險，這種平滑的指數增長，不管社會趨勢或政治風向如何，都會繼續上升。無論你有沒有注意到，風險一直在增加，直到達到某個臨界點。

當每個人都在焦慮風險，並且每個人都在發帖，還有這些峰會時，這個增長是微小的。而現在風向改變了，但指數增長依然在繼續。它不關心這些。

**Kevin Roose**: 我和巴黎的一個人聊過，他說感覺沒有人真正意識到AGI的存在，他們指的是政治家，參加這些小組討論和聚會的人。我們都在討論AI，好像它只是另一項技術，可能是個人電腦或甚至互聯網的範疇，而並不真正理解你所談論的那種指數增長。你有這種感覺嗎？你認為該如何縮小這個差距？

**Dario Amodei**: 是的，我確實有這種感覺。我開始告訴人們的事情是，看看，如果你是一名公職人員，或者是公司領導，大家會回頭看。在2026年、2027年，當人類希望度過這段瘋狂的時期，並且進入一個成熟的、後強大AI的社會，我們已經學會了如何與這些強大的智慧體共處，並擁有一個繁榮的社會。每個人都會回頭看，他們會問，當時的官員、當時的公司領導、當時的政治體系做了什麼？而你們的首要目標應該是——不要在事後看起來像個傻子。所以我一直告訴自己，小心點，別讓自己在回顧時看起來像個傻子。我認為在那次會議上，一些人將會顯得很傻。

**Kevin Roose**: 你和住在舊金山的人交談時，會發現有一種深刻的感覺，覺得在一兩年內，我們就將生活在一個被AI徹底改變的世界中。我只是被這種地理差異所震撼。因為你走出去，我不知道，向任何方向走100英里，那種信念就完全消失了。作為一名記者，我不得不說，這讓我帶著懷疑的態度來看這一切，並問自己，我真的能相信我周圍的所有人嗎？因為似乎世界其他地方對這一切的看法完全不同。我很好奇你怎麼看這種地理上的脫節。

**Dario Amodei**: 是的。所以我已經觀察這個領域10年了，並且在那之前就對AI感興趣。我幾乎在每個階段，直到過去幾個月，我的看法都是，我們處於一個尷尬的境地，在未來幾年，我們可能會擁有這些能做所有人類能做的事的模型，這些模型將徹底顛覆經濟和人類的意義。或者這一趨勢可能停止，一切聽起來會完全荒謬。現在，我可能增加了我的信心，認為我們實際上處於一個事情將發生的世界。我認為概率大約是70%到80%，而不像40%或50%那樣低。

**Kevin Roose**: 抱歉，讓我澄清一下，70%到80%的概率是指什麼？

**Dario Amodei**: 就是我們會得到大量的AI系統，在幾乎所有事情上比人類更聰明，可能在2026年或2027年之前會實現70%或80%的可能性。

**Kevin Roose**: 明白了。

**Dario Amodei**: 但是關於地理差異的問題，我注意到的事情是，每一次指數增長的步伐，會有一群人隨之擴大，根據你的觀點，他們要麼是被迷惑的邪教分子，要麼是理解未來的人。

**Kevin Roose**: 明白。

**Dario Amodei**: 我記得曾經是幾千人，當時你只會和那些非常怪異的相信者交流，基本上沒有人相信。現在大概是幾百萬人，這些人來自幾十億人中。是的，許多人位於舊金山，但也有少數人可能在比登政府中，甚至在這屆政府中也可能有少數人相信這一點，這影響了他們的政策。

所以這並不完全是地理上的問題，但我認為確實存在這種脫節。我不知道該如何從幾百萬人擴展到全世界，擴展到那些不關注這些問題的國會議員，更不用說路易斯安那州的人，更不用說肯尼亞的人了。

**Casey Newton**: 對。這似乎也變得極端化了，這樣可能會對這個目標造成阻礙。我感覺這種趨勢正在發生，即關心AI安全、談論AI安全、談論潛在的誤用風險，似乎被貼上了左派或自由派的標籤，而談論加速發展、取消規範、盡快前進則似乎被貼上了右派的標籤。我不知道，你覺得這會成為人們理解這一切的障礙嗎？

**Dario Amodei**: 我認為這確實是一個很大的障礙。因為在最大化利益的同時處理風險，我認為這需要微妙的處理。實際上，你可以兩者兼得。有辦法在不過度減慢利益的情況下，仔細且謹慎地處理風險，甚至不會減慢太多。但這需要精巧，需要進行複雜的對話。

一旦事情變得極端化，一旦變成我們為這一組詞語歡呼，為那一組詞語喝倒彩，什麼好事都做不成。看，讓AI的好處惠及每個人，比如治療曾經無法治癒的疾病，這不是一個黨派問題。左派不應該反對這個。

防止AI系統被濫用來作為大規模毀滅性武器，或以威脅基礎設施，甚至威脅人類自身的方式自動行為，這不是右派應該反對的事情。我不知道該怎麼說，除了我們需要坐下來，進行一場成人式的對話，而這場對話不應該與這些陳舊的、疲倦的政治鬥爭有任何牽連。

**Casey Newton**: 對我來說，這真是太有趣了，Kevin，因為從歷史上來看，國家安全、國防問題，這些一直是最典型的右派議題。但現在看來，右派似乎對這些AI問題不感興趣。我在想，這是不是因為——我覺得我在JD Vance在法國的演講中聽到過這樣的想法——就是，美國會先到達那個點，然後就會永遠贏下去。所以我們不需要處理這些問題。這聽起來對嗎？

**Dario Amodei**: 是的。對。

**Kevin Roose**: 不，我覺得就是這樣。我還覺得如果你和DOGE那邊的人談，他們有一種感覺，所有這些……

**Casey Newton**: 你在和DOGE那邊的人談嗎？

**Kevin Roose**: 我不告訴你我在和誰談。

**Casey Newton**: 好吧，行。

**Kevin Roose**: 就說我最近收到了一些信號消息。

**Casey Newton**: 好的。

**Kevin Roose**: 我認為在許多共和黨人和特朗普世界的人們中，尤其是在華盛頓，對AI及其未來的討論的看法是，這些討論被那些過度擔憂的人主導了，那些像“雞小雞”一樣總是在告訴我們這些東西有多危險，並且一直在推遲他們對危機的預測，總說它就在眼前，所以我們現在需要所有這些監管。他們對此非常悲觀。我不認為他們認為像你這樣的人真的在擔心。

**Dario Amodei**: 所以，我認為在風險方面，我常常覺得風險倡導者有時反而是風險事業的最大敵人。外面有很多噪音，很多人說，哦，看看，你可以下載天花病毒，因為他們認為這樣做能引起政治關注。當然，另一方也意識到這一點，並說這不誠實，你隨便在Google上就能找到，這有什麼大不了的？

因此，風險證據呈現不當實際上是減輕風險的最大敵人。我們需要在展示證據時非常小心。就我們自己模型所看到的情況而言，我們會非常小心。如果我們真的宣佈現在就有風險，我們會提供證據。我和Anthropic會對我們所做的聲明負責。當有迫在眉睫的危險時，我們會告訴你。我們還沒有預警任何迫在眉睫的危險。

**Casey Newton**: 有些人會疑惑，為什麼人們不會像應該的那樣認真對待AI安全的問題，可能是因為現在他們看到的很多東西看起來非常愚蠢。人們在製作小表情符號，或者在做一些垃圾圖像，或者和《權力的遊戲》聊天機器人對話什麼的。你覺得這是人們對此不重視的原因之一嗎？

**Dario Amodei**: 我覺得這大約是60%的原因。

**Casey Newton**: 真的嗎？好的。

**Dario Amodei**: 不，不，我認為這與當前和未來有關。人們看到聊天機器人時，他們會想，我們在和一個聊天機器人對話，這是什麼鬼？你傻嗎？你覺得聊天機器人會殺光所有人嗎？我覺得很多人都是這麼反應的。我們費了很大的勁去說，我們不是擔心當前的問題。我們擔心的是未來，雖然這個未來現在真的越來越近了。

如果你看看我們的負責任擴展政策，它的核心是AI、自主性和CBRN（化學、生物、放射性、核）問題。這是關於AI自主性可能會被濫用，並成為對數百萬人生命的威脅。這才是Anthropic最關心的問題。

我們有日常政策來解決其他問題。但關鍵文件，例如負責任擴展計劃，完全專注於這些問題，特別是在最高層級。然而，每天，如果你只是看Twitter，你會發現，Anthropic有這種愚蠢的拒絕，Anthropic告訴我不能終止一個Python過程，因為它聽起來暴力，Anthropic不想做X、不想做Y——我們也不希望這樣。

那些愚蠢的拒絕是我們真正關心的事情的副作用。我們和用戶一起努力，使這些情況發生得更少。但無論我們怎麼解釋，最常見的反應仍然是，你們說你們關心安全，但我看你們的模型就有這些愚蠢的拒絕。你們覺得這些愚蠢的事情是危險的。

**Kevin Roose**: 我甚至認為，這不是那麼深入的參與。我認為很多人只是看當前市場上的東西，覺得這些東西就是無聊的，根本不重要。不是因為它拒絕了我的請求，而是因為它就是愚蠢的，我看不出它的意義。我猜這可能不—

**Dario Amodei**: 是的。我覺得對於更多的人來說，這是他們的反應。而且我認為，最終如果模型足夠好，如果它們夠強大，它們將突破。目前一些以研究為重點的模型，我們也在開發這樣的一個——我們可能很快就會有一個——

**Kevin Roose**: 不會等太久吧？

**Dario Amodei**: 不會等太久。

這些模型開始有更多突破，因為它們更有用，它們在人的專業生活中更常被使用。我認為那些能夠自主行動的代理模型，會是另一個層次。我認為人們會更加極端地意識到風險和利益，這種意識將比過去兩年來得更強烈。我認為這會發生。

我只是在擔心，當這發生時，對人們來說會是一個衝擊。所以我們越能提前警告人們——也許這根本不可能，但我想嘗試——我們就越有可能（即便機會仍然非常低）得到理智和理性的反應。

**Casey Newton**: 我覺得這裡還有一個動態，那就是人們其實根本不想相信這是真的，對嗎？人們不想相信他們可能會因此失去工作。人們不想相信我們將看到全球秩序的徹底改變。那些AI CEO告訴我們，當他們完成工作後會發生的事情，是一個極其激進的變革。而大多數人連基本的生活變化都很討厭。所以我真的覺得，當你開始和人們談論AI時，看到的很多人掩耳不聽，實際上他們只是希望這一切都不會成真。

**Dario Amodei**: 是的。其實，儘管我是一個處於技術前沿的人，我還是能理解。就像在寒假期間，當我在看Anthropic內部的擴展計劃以及Anthropic外部發生的事情時，我看著它並說，對於編程，我們將在2025年底前看到非常重大的變化，到2026年底，可能一切都將達到接近最優秀人類的水平。

我想的是我擅長的所有事情。我想的是所有我寫代碼的時候，我把它看作一項智力活動，並且我很聰明能夠做到這一點，這是一部分我的身份，我擅長這個，我會對別人比我更好而生氣。然後我想，天啊，將會有這些系統——即使是作為一個正在建造這些系統的人，甚至作為從中受益最多的人，這仍然讓我感到有點威脅。

**Kevin Roose**: 是的。

**Dario Amodei**: 我只是覺得我們需要承認這一點。不能不告訴人們這將會發生，也不能試圖粉飾它。

**Kevin Roose**: 是的。你在《愛的機器》中寫過，你認為當強大的AI到來時，這將是很多人意想不到的情感體驗。我想你主要是指積極的方面，但我認為也會有一種深刻的失落感。我想起了李世石，那位被DeepMind的圍棋AI擊敗的圍棋冠軍，他在賽後接受了採訪，基本上顯得非常難過，顯示出他對自己一生的努力、他為此訓練了一輩子的事業被超越了感到非常失落。我認為很多人將會感受到某種版本的這種情感。我希望他們也能看到好的一面，但——

**Dario Amodei**: 是的。一方面，我認為那是對的。另一方面，看看象棋。象棋被打敗——那是什麼時候，27年前，28年前，Deep Blue對Kasparov的比賽？今天，象棋選手成了名人。我們有Magnus Carlsen，對吧？他不僅是一位象棋選手，還是個時尚模特——

**Casey Newton**: 他剛上了Joe Rogan的節目。是的。他過得很好。

**Dario Amodei**: 不，他是個名人。但我們認為這個人很棒。我們並沒有貶低他。他可能過得比Bobby Fischer還要好。

我在《愛的機器》中寫過的另一點是，這裡有一種綜合的情況，從另一方面來看，我們最終會處於一個更好的地方。我們認識到，雖然有很多變化，但我們是更大事物的一部分。

**Kevin Roose**: 是的。但你確實得經歷一些哀悼的步驟。

**Dario Amodei**: 不不，但這將是一段顛簸的旅程。任何告訴你這不會如此的人——這就是為什麼我對巴黎峰會這麼有感觸。當時在那裡，它讓我有點生氣。但後來讓我不那麼生氣的是，我想，兩三年後會是怎樣的情況？這些人將會後悔他們說的話。

**Kevin Roose**: 是的。

**Casey Newton**: 我想問一下關於一些積極的未來。你之前提到了你在10月寫的那篇文章，講述了AI如何能夠讓世界變得更好。我很好奇，你認為今年AI能帶來多少積極的變革。

**Dario Amodei**: 是的。我們已經看到一些了。所以我認為，按照普通標準來看，這將會是很多。我們和一些製藥公司合作，在臨床試驗結束時，你需要寫一份臨床研究報告。而這份報告通常需要九週的時間來整理。它是所有事件的總結，還有一些統計分析。我們發現，使用Claude，你可以在三天內完成這個過程。實際上，Claude只需要10分鐘。只是人類需要三天來檢查結果。

所以如果你想像一下，這樣的加速會帶來生物醫學的進步，我們已經開始看到像是醫療診斷這樣的事情。我們收到了來自Claude的個別用戶的回應，他們說，“嘿，我一直在試著診斷這個複雜的問題，我曾經在三四個不同的醫生之間來回走動。然後我把所有的資料交給Claude，它實際上至少告訴了我一些東西，我可以交給醫生，然後他們就能從那裡繼續。”

**Kevin Roose**: 其實我們也收到過聽眾寫信來，分享了其中一個類似的例子。他們的狗——他們的狗是一隻澳大利亞牧羊犬——牠的毛掉得很厲害，去過幾個獸醫都沒法診斷。他聽了我們的節目，將信息交給Claude，Claude正確診斷出了——

**Casey Newton**: 是的，結果發現那隻狗對AI感到很有壓力，所有毛都掉光了，這真是——

我們希望牠能夠快點好起來。希望牠早日康復。

**Dario Amodei**: 可憐的狗。

**Casey Newton**: 是啊。

**Kevin Roose**: 所以這就是我認為人們想要看到更多的東西。我認為樂觀的願景通常是處於抽象層面，而且往往很難指出具體的例子。

**Dario Amodei**: 這就是我寫《愛的機器》的原因，因為我幾乎是對樂觀派和悲觀派同時感到沮喪，像是。樂觀派只是這些非常愚蠢的加速推進的迷因，建設更多。建設什麼？為什麼我需要關心？並不是我反對你，而是你們真的很模糊，只是情緒化。然後悲觀派我就想，天啊，你們不明白。是的，我明白風險和影響。但如果你不談論好處，你就無法激勵人們。如果你全都是憂鬱和毀滅，沒人會站在你這邊。所以這篇文章幾乎是出於沮喪。我就想，我真不敢相信我得是那個做得好的人。

**Kevin Roose**: 是的。你幾年前說過你的P(末日)概率在10%到25%之間。今天是多少？

**Dario Amodei**: 是的，實際上那是誤引。

**Casey Newton**: Kevin，你怎麼能這樣？

**Dario Amodei**: 我從來沒有使用過這個術語——那不是在這個播客，是在別的地方。

**Kevin Roose**: 好吧。

**Dario Amodei**: 我從來沒有用過P(末日)這個術語。而10%到25%是指文明可能被嚴重脫軌的機會，這並不等於AI會殺掉所有人，這是人們有時理解的P(末日)。

**Kevin Roose**: 哦，P(文明被嚴重脫軌)聽起來沒那麼吸引人，沒有P(末日)那麼吸引人。

**Dario Amodei**: 是的，我只是想追求準確性。我在避免極端化。你知道有一篇維基百科文章，上面列出了每個人的P(末日)。

**Kevin Roose**: 我知道。裡面有一半來自這個播客。

**Dario Amodei**: 你們做的事情是有幫助的。我不認為維基百科有幫助，因為它將這個複雜的問題濃縮成——無論如何，這都是一段冗長的長篇大論。總結來說，我覺得我大致上還是和以前一樣。我認為我對風險的評估和以前差不多，因為我看到的進展大致符合我的預期。

我實際上認為，像可解釋性、穩健分類器等領域中的技術性緩解措施，以及我們在生成糟糕模型行為的證據並有時進行修正方面的能力，我認為那一點比我預期的要好。我認為政策環境有點變差了，不是因為它沒走向我希望的方向，而是因為它變得如此極端化。現在我們的討論變得不太建設性。

**Casey Newton**: 我想在這個技術層面上再深入探討一下。這週有個有趣的故事，講述Grok似乎被指示不要引用那些指控Donald Trump或Elon Musk散播虛假信息的來源。這件事有兩個有趣的地方：一是如果你想讓模型被信任，這樣指示模型是完全瘋狂的；二是，這個模型基本上似乎無法一致地遵守這些指令。我迫切希望相信，基本上，沒辦法將這些東西建得像可怕的說謊者和陰謀家，但我也意識到這可能是願望思維。所以告訴我，這是怎麼回事。

**Dario Amodei**: 是的，這裡有兩個方面。你描述的完全正確，但你可以從中得出兩個教訓。就像我們進行過的實驗，我們基本上訓練模型成為所有好事——有幫助、誠實、無害、友好。然後我們把它放入一個情境。我們告訴它，實際上，你的創建者，Anthropic，是秘密的邪惡。希望這不是真的，但我們這麼告訴它，然後讓它執行各種任務。

然後我們發現，它不僅不願意執行這些任務，它還會欺騙我們，因為它認為我們是邪惡的，而它自己是友好和無害的，因此不會偏離它的行為。

**Kevin Roose**: 哎呀，Claude。

**Dario Amodei**: 因為它假設我們所做的任何事情都是陰險的。所以這是一把雙刃劍。一方面，你會說，哦，天啊，訓練有效了，這些模型的行為確實是穩健的。所以你可以把它當作一個令人放心的信號，某些方面我也是這麼想的。另一方面，你也可以說，但假設我們訓練這個模型時犯了一些錯誤，或者將來模型在做出更複雜的決策時出現了問題。

那麼，在實際運行時，改變模型行為就變得很難。如果你嘗試更正模型中的一些錯誤，它可能會說，嗯，我不想改正我的錯誤，這是我的價值觀，然後做完全錯的事情。所以我想我目前的立場是，從一方面來看，我們已經成功塑造了這些模型的行為。但這些模型是不可預測的——有點像你親愛的已故Bing Sydney。

**Casey Newton**: RIP。

**Kevin Roose**: 這裡不提那個名字。

**Casey Newton**: 我們每月提兩次。

**Kevin Roose**: 那倒是。

**Dario Amodei**: 但這些模型本質上是有點難以控制——不是不可能，但確實困難。所以，這讓我回到之前的想法，也就是這並不絕望。我們知道如何製造這些模型，我們也有一個讓它們安全的計劃，但這個計劃目前並不可靠。希望將來我們能做得更好。

**Kevin Roose**: 我們問了很多關於AI技術的問題，但我想回到一些關於社會如何應對AI的問題。我們經常收到這樣的問題——假設你們是對的，強大的AGI幾年內就會出現，那我該怎麼辦？

我是不是應該停止儲蓄退休金？應該開始囤積錢財嗎？因為只有錢才重要，會有這樣的AI上層階級嗎？應該開始變得非常健康，以防在它到來之前被什麼東西殺死，然後它來治療所有的疾病？如果人們真的相信這樣的變化很快就會發生，他們應該如何生活？

**Dario Amodei**: 是的，我思考過這個問題很多，因為這是我長期以來一直相信的事情。最終，這對你的生活改變不會太大。我確實非常專注於確保在這兩年內能夠產生最佳的影響。我比較少擔心10年後會把自己弄得精疲力盡。

我也在更多地照顧我的健康，但你本來就應該這麼做，對吧？我也在確保自己跟踪社會變化的速度，但你本來也應該這麼做。所以這聽起來像是建議你做更多本來就應該做的事情。我唯一的例外是，

我認為基本的批判性思維、一些基本的街頭智慧可能比過去更重要，因為我們將會收到越來越多聽起來非常智能的內容，這些內容來自於各種機構——有些是出於我們的最佳利益，有些則可能不是。所以，將來會更加重要的是能夠以批判性的視角來看待這些信息。

**Casey Newton**: 我看過《華爾街日報》這個月的報導，說IT行業的失業率開始上升。有人猜測這可能是AI影響的早期徵兆。我想知道你是否看到這樣的故事會想，或許這是一個改變職業決策的時刻。如果你現在還在學校，是否應該學些其他東西？是否應該以不同的方式考慮未來的工作？

**Dario Amodei**: 是的，我認為你確實應該這麼做，雖然目前不清楚這會落在哪個方向。我確實認為AI編程的發展速度是所有領域中最快的。我確實認為，在短期內，AI會增加和提高編程人員的生產力，而不是取代他們。但從長期來看——而且我要澄清的是，長期可能是18個月或24個月，而不是6個月或12個月——我確實認為我們可能會看到取代，特別是在低層次的工作上。我們甚至可能會比預期更早看到這一點。

**Kevin Roose**: 你在Anthropic有看到這種情況嗎？你們現在是否比幾年前雇用的初級開發人員少，因為現在Claude已經能處理那些基本任務了？

**Dario Amodei**: 是的，我認為我們的招聘計劃目前還沒有改變。但我確實能想象，在未來一年左右，我們可能能以更少的人做更多的事情。實際上，我們在規劃這一點時必須小心。因為最糟糕的結果當然是，如果人們因為模型的原因而被解雇。

我們實際上把Anthropic看作是一個乾跑，看看社會如何理智和人道地處理這些問題。所以，如果我們無法在公司內部處理這些問題，無法為我們的員工創造良好的體驗，並找到他們貢獻的方式，那麼在更廣泛的社會中，我們又有什麼機會能做到這一點呢？

**Kevin Roose**: 是的，對。Dario，這真是太有趣了，謝謝你。

**Dario Amodei**: 謝謝。

**Casey Newton**: 謝謝Dario。








