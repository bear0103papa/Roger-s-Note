---
layout: post
title: "AI Semiconductor Landscape feat. Dylan Patel | BG2 w/ Bill Gurley & Brad Gerstner"
date: 2024-12-13
categories: [Semiconductor, AI]
description: "SemiAnalysis創辦人Dylan Patel與Bill Gurley和Brad Gerstner討論AI半導體產業格局，探討NVIDIA的競爭優勢、AI模型訓練的發展趨勢以及產業未來展望。"
---
<span class="original-link">原文連結： [BG2 Podcast - AI Semiconductor Landscape feat. Dylan Patel](https://www.youtube.com/watch?v=QVcSBHhcFbg) </span>

AI Semiconductor Landscape feat. Dylan Patel | BG2 w/ Bill Gurley & Brad Gerstner

Brad Gerstner 一位美國投資者和對沖基金經理人

Bill Gurley 他是位於加州舊金山的矽谷創投公司 Benchmark 的普通合夥人

SemiAnalysis 創辦人 Dylan Patel

Highlight

你認為目前全球AI工作負載中有多少百分比是在NVIDIA的芯片上運行？"

"如果不算上Google的話，會超過98%。但如果把Google算進來，實際上更接近70%，因為Google在AI工作負載中佔據了很大比例，特別是在生產環境中的工作負載。當談到'生產'時，也就是能賺錢的項目，這個比例可能甚至低於70%。因為想想看，Google搜索和Google廣告是世界上最大的兩個AI驅動的業務，只有TikTok和Meta的業務能與之相提並論。"

"為什麼NVIDIA如此主導市場？我喜歡把它比喻成一條三頭龍。首先，世界上所有半導體公司在軟件方面都很糟糕，除了NVIDIA。所以有軟件這一塊，當然還有硬件。人們沒有意識到NVIDIA在硬件方面實際上比大多數人想像的要強得多。他們總是最先接觸到最新技術，因為他們瘋狂地追求達到特定的生產目標，他們從構思到部署的速度比其他人都快。然後是網絡方面，他們收購了Mellanox，在網絡方面也投入了大量精力。這三個方面結合在一起，形成了一條其他半導體公司單獨無法匹敵的三頭龍。"

晶片需求

"當你廣泛地看GPU時，沒有人會只買一個芯片來運行AI工作負載。模型的規模已經遠遠超出了這個範圍。看看今天最先進的模型，比如GPT-4，有超過萬億個參數。一萬億參數需要超過1TB的內存，你不可能在一個芯片上獲得這樣的容量，一個芯片也不可能有足夠的性能來服務這個模型，即使它有足夠的內存容量。因此，你必須將多個芯片連接在一起。"

"有趣的是，NVIDIA看到了這一點，並建立了一個名為NVLink的架構，可以很好地將多個芯片網絡連接在一起。但有趣的是，很多人忽略了Google其實是與Broadcom一起在NVIDIA之前就做到了這一點。今天大家都在興奮地談論NVIDIA的Blackwell系統，它是一個作為購買單位的GPU機架，不是一台服務器，不是一個芯片，而是一個機架。這個機架重達三噸，有數千根電纜，這些都是Jensen可能會告訴你的。有趣的是，Google在2018年就用TPU做了類似的事情。"

"我認為NVIDIA在差異化方面主要關注供應鏈相關的事情。這可能聽起來像是'哦，他們只是在訂購東西'，不，不，不，你必須與供應鏈深度合作，開發下一代技術，這樣你才能在任何人之前將它推向市場。因為如果NVIDIA停滯不前，他們就會被吞噬。"

"對NVIDIA來說，主要是這樣的 - 這個工作負載規模很大，已經超過1000億美元的支出。對於最大的客戶來說，他們有多個客戶每年花費數十億美元。我可以雇用足夠多的工程師來弄清楚如何在其他硬件上運行我的模型。也許我現在還不能弄清楚如何在其他硬件上訓練，但我可以弄清楚如何在其他硬件上進行推理。所以NVIDIA在推理方面的軟件護城河實際上小得多，但他們在硬件方面的優勢更大，因為他們就是有最好的硬件。"

"最好的硬件是什麼意思？這意味著資本成本、運營成本和性能。性能TCO(總擁有成本)是關鍵。NVIDIA的整個護城河在於，如果他們停滯不前，他們的性能TCO就不會增長。但有趣的是，他們確實在增長。比如Blackwell，它不僅速度快得多，在非常大的模型推理方面快了10到15倍，因為他們針對大型語言模型進行了優化，他們還決定要降低一些利潤率，因為他們在與亞馬遜的芯片、TPU、AMD等競爭。"

"所有這些軟件都極其難以複製。事實上，除了大型科技公司外，沒有其他人有部署能力。A100 GPU的規模就像是微軟的推理集群，而不是訓練集群。所以當你談論這裡的難度時，在訓練方面，這是用戶不斷實驗，研究人員說'讓我們試試這個，試試那個'，我沒有時間去優化和手動調整性能，我依賴NVIDIA的性能在現有軟件堆棧下或很少努力就能達到很好的效果。"

"我現在可以升級 - 亞馬遜數據中心中最多的CPU是2015年到2020年間製造的24核Intel CPU，基本上是相同的架構。這個24核CPU，我現在可以買128核或192核的CPU，每個CPU核心的性能都更高。如果我用一台服務器替換六台，我基本上就憑空創造了電力，對吧？因為這些超過六年的舊服務器可以報廢了，用新服務器的資本支出，我可以替換這些舊服務器，現在每次我這樣做，我就可以再放入一台AI服務器。"

"所以這就是某種程度的替換。我仍然需要更多的總容量，但這個總容量可以由更少的機器提供，如果我買新的話。一般來說，市場不會萎縮，它仍然會增長，只是遠不及AI的增長。而AI正在導致這種行為 - 我需要替換以獲得電力。"

資料中心正在加速如果你查看所有不同的監管文件、衛星圖像以及我們所做的所有分析，你就能看出他們將會有多少數據中心容量。這個趨勢正在加速，那麼這些空間要用來做什麼呢？事實證明，要填滿這些空間，你可以根據每個GPU的總功耗做一些估算。雖然satya說他要放緩一點，但他們已經簽署了明年的租約。他說部分原因是他預計明年上半年的雲端營收會加速增長，因為他們將有更多的數據中心容量，而目前正受到容量限制。

今天Claude的成本非常高，你降級到GPT-4就便宜很多，再降到GPT-4 Mini就更便宜了。為什麼？因為現在使用GPT-4 Mini，我要與Llama競爭，要與DeepSeek競爭，要與Mistral競爭，要與阿里巴巴競爭，還要與其他許多競爭者競爭。

現在有15家不同的公司在Llama、阿里巴巴、DeepSeek和Mistral等不同模型上提供API推理服務。

這些公司的利潤率要低得多。微軟在開源模型上賺取50-70%的毛利率，這還包括他們必須分享的利潤。同樣，Anthropic在他們最近一輪融資中也展示了70%的毛利率，但那是因為他們有這個模型。

這就是美妙之處 - 一切都是商品化的嗎？不是，但如果你在部署服務，這對你來說是很好的。

如果你是實驗室之一，你必須擁有最好的模型

第三個原因是Google內部使用TPU更有利可圖。順便說一下，微軟實際上租用的GPU很少。他們從內部工作負載或用於推理獲得的利潤更高，因為銷售tokens的毛利率在50-70%，而出租GPU服務器的毛利率低於這個數字。雖然這也是不錯的毛利率，但在他們引述的100億美元收入中，沒有任何部分來自外部GPU租賃。

所以我認為你很快就會發現，在模型方面，只有少數幾家公司能夠在這個市場上競爭。是的，我認為在誰願意為這些模型付費方面，我認為會有更多人願意為最好的模型付費。

首先，博通確實贏得了多個定制ASIC訂單，不僅僅是Google。Meta正在擴大主要用於推薦系統的定制晶片，他們的定制晶片會變得更好。還有其他玩家，如正在製作晶片的OpenAI，以及蘋果(雖然不是完全與博通合作，但有一小部分會與博通合作)。他們現在有很多訂單，這些不會都在2025年實現，有些要到2026年。

我長期看好博通，但在接下來的六個月內，由於Google沒有數據中心空間，TPU採購會有所放緩。他們想要更多，但確實沒有地方放置。我們可以看到會有一個暫停期，但人們可能會忽視這一點。

我認為超大規模雲端服務商明年的支出計劃相當明確 - 他們會花費更多。因此，整個生態系統，包括網路設備商、ASIC供應商、系統供應商都會表現良好，無論是英偉達、Marvell、博通還是AMD，雖然有些會比其他表現更好。

問題在於人們是否會繼續保持這樣的支出水平，這取決於模型是否會持續進步。如果模型停止進步，我認為會出現一個重大的事件。

但這不會發生在明年。事實上，我認為明年模型會進步得更快。

我們追蹤並與80家新興雲服務商交談，了解他們擁有多少GPU。問題是現在H100的租賃價格正在下跌，不僅是在這些新興雲服務商那裡。過去你必須簽4年合約並預付25%，簽訂一輪融資然後購買一個集群，僅此而已。現在你可以以比過去4年期合約更好的價格獲得3個月或6個月的合約。

不僅如此，亞馬遜的隨需GPU價格也在下跌。雖然相對來說仍然很貴，但價格正在快速下跌。80家新興雲服務商不可能都能存活，可能只有5到10家能活下來。因為其中5家是主權雲，另外5家是真正具有市場競爭力的。

那麼擴展是否已經結束了？為什麼Mark Zuckerberg還在路易斯安那州建造2吉瓦的數據中心？為什麼亞馬遜在建造這些吉瓦級的數據中心？為什麼谷歌、為什麼微軟要建造多個吉瓦級的數據中心，還要花費數十億美元購買光纖來連接它們？因為他們認為需要在規模上取勝，所以要將所有數據中心用超高帶寬連接在一起，讓它們能像一個數據中心一樣運作。所以當你看到那些最了解情況的人在投入什麼時，這個"擴展已經結束"的說法就不攻自破了。

[音樂]

[掌聲]很高興來到這裡。Dylan，這是我們今年一直在討論的事情之一，就是計算世界正在發生根本性的變革。Bill，你來給大家介紹一下Dylan是誰，然後我們開始吧。

是的，我們很高興邀請到來自Semi Analysis的Dylan Patel。Dylan已經迅速建立起了全球半導體行業最受尊重的研究團隊。我們今天想要深入探討的是，從技術角度來看，Dylan對於現有架構、擴展、全球主要參與者、供應鏈的了解，以及最優秀的人才都在聆聽和閱讀Dylan的工作，然後將其與我們的觀眾關心的商業問題聯繫起來，看看會有什麼結果。

我希望能夠對所有與這波AI浪潮相關的半導體活動做一個當前的快照，並嘗試將其放在正確的視角中。Dylan，你是如何進入這個領域的？

當我8歲時，我的Xbox壞了。我的父母是移民，我在喬治亞州鄉下長大，除了當個書呆子外沒什麼可做的。我不能告訴他們我弄壞了Xbox，所以我必須打開它，把溫度感應器短路來修好它。當時我並不知道自己在做什麼，但後來我就留在那些論壇上，成了一個論壇戰士。你知道那些在評論區總是對你大喊大叫的人，Brad，那就是小時候的我。作為一個孩子你當時並不知道，但後來當我開始賺錢後，我就開始閱讀半導體公司的財報並投資它們，用我實習賺來的錢。當然也閱讀技術相關的內容，然後工作了一段時間，就是這樣。

談談Semi Analysis現在的業務情況吧。我們是一家半導體研究公司，也是AI研究公司。我們服務的客戶包括所有大型科技公司、最大的半導體公司、私募基金和對沖基金。我們銷售關於全球每個數據中心的數據，包括它們的功率、每季度的建設情況。我們追蹤全球所有1500個晶圓廠的情況 - 雖然就你的目的而言，只有50個是真正重要的。我們還追蹤整個供應鏈，無論是電纜、服務器、電路板還是變電站設備，我們都試圖用非常數據驅動的方式來追蹤這一切，並進行預測。然後我們在這些領域提供諮詢服務。

Bill，你和我剛才談到這個。我是說，對於我們的團隊來說，我們一直在與Dylan和他的團隊交流。我認為你說得對，他真的就是通過努力工作、做那些重要但繁瑣的事情，迅速成為了半導體行業的標杆。我們現在可能已經進入這個建設期的第二年，發展非常迅速。Bill和我正在討論的一件事是，當我們進入2024年底時，需要深呼吸一下，思考2025年、2026年及以後的發展。因為很多事情都在改變，有很多爭論，這將影響到公共市場和私人市場數萬億美元的價值，以及大型科技公司如何投資和我們未來的發展方向。

Bill說："如果要談AI和半導體，那就必須從NVIDIA開始。Dylan，你認為目前全球AI工作負載中有多少百分比是在NVIDIA的芯片上運行？"

"如果不算上Google的話，會超過98%。但如果把Google算進來，實際上更接近70%，因為Google在AI工作負載中佔據了很大比例，特別是在生產環境中的工作負載。當談到'生產'時，也就是能賺錢的項目，這個比例可能甚至低於70%。因為想想看，Google搜索和Google廣告是世界上最大的兩個AI驅動的業務，只有TikTok和Meta的業務能與之相提並論。"

這些Google的工作負載，我認為重要的是要說明一下，這些是運行在Google自己的專有芯片上，它們是非LLM工作負載，對嗎？

"是的，Google的生產環境中的非LLM和LLM工作負載都運行在他們的內部硅片上。有趣的是，雖然大家都說Google在Transformer和LLM方面失誤了，為什麼是OpenAI做出了GPT而不是Google，但實際上Google從2018-2019年就開始在他們的搜索工作負載中使用Transformer了。在我們進入GPT狂熱之前，BERT是最著名、最流行的Transformer之一，它已經在他們的生產搜索工作負載中使用多年了。"

回到這個數字，你說如果只看企業購買來做自己工作的工作負載，去掉那些內部使用的，就是98%，對吧？這在當前時間點確實是壓倒性的優勢。

再談談Google，他們也是NVIDIA的大客戶之一，他們確實購買了不少GPU。他們購買一些用於YouTube視頻相關的工作負載和內部工作負載。所以不是所有內部工作都是用TPU。他們確實購買一些用於其他內部工作負載，但他們購買GPU主要是為了Google Cloud，然後租給客戶使用。雖然他們確實有一些客戶使用他們的內部硅片，比如蘋果，但他們對外的雲業務中的AI服務，絕大多數還是使用GPU，而且是NVIDIA的GPU。

"為什麼NVIDIA如此主導市場？我喜歡把它比喻成一條三頭龍。首先，世界上所有半導體公司在軟件方面都很糟糕，除了NVIDIA。所以有軟件這一塊，當然還有硬件。人們沒有意識到NVIDIA在硬件方面實際上比大多數人想像的要強得多。他們總是最先接觸到最新技術，因為他們瘋狂地追求達到特定的生產目標，他們從構思到部署的速度比其他人都快。然後是網絡方面，他們收購了Mellanox，在網絡方面也投入了大量精力。這三個方面結合在一起，形成了一條其他半導體公司單獨無法匹敵的三頭龍。"

Dylan，你寫過一篇文章，幫助大家visualize現代前沿NVIDIA部署的複雜性，涉及機架、內存、網絡以及整個系統的規模，非常有幫助。人們經常把純粹的芯片公司與NVIDIA進行比較，但它們不是系統公司，不是基礎設施公司。我認為人們深深低估了NVIDIA的競爭護城河。軟件在提高效率和降低運營總成本方面變得越來越重要。

談談Bill提到的那個架構圖，你知道，系統架構有很多不同的層次，這與定制ASIC或AMD有什麼區別？

"當你廣泛地看GPU時，沒有人會只買一個芯片來運行AI工作負載。模型的規模已經遠遠超出了這個範圍。看看今天最先進的模型，比如GPT-4，有超過萬億個參數。一萬億參數需要超過1TB的內存，你不可能在一個芯片上獲得這樣的容量，一個芯片也不可能有足夠的性能來服務這個模型，即使它有足夠的內存容量。因此，你必須將多個芯片連接在一起。"

"有趣的是，NVIDIA看到了這一點，並建立了一個名為NVLink的架構，可以很好地將多個芯片網絡連接在一起。但有趣的是，很多人忽略了Google其實是與Broadcom一起在NVIDIA之前就做到了這一點。今天大家都在興奮地談論NVIDIA的Blackwell系統，它是一個作為購買單位的GPU機架，不是一台服務器，不是一個芯片，而是一個機架。這個機架重達三噸，有數千根電纜，這些都是Jensen可能會告訴你的。有趣的是，Google在2018年就用TPU做了類似的事情。"

"當然，他們不能獨自完成。他們知道軟件，知道計算元素需要什麼，但他們不能做很多其他困難的事情，比如封裝設計、網絡等，所以他們必須與其他供應商如Broadcom合作。因為Google對AI模型的發展方向有如此統一的願景，他們實際上能夠建立一個針對AI優化的系統架構。而當時NVIDIA還在考慮要擴展到多大規模。我相信他們本可以嘗試擴展得更大，但他們看到主要工作負載並不需要擴展到那種程度。現在每個人都看到了這一點，都在朝這個方向發展，但NVIDIA已經有了Blackwell即將推出。競爭對手如AMD等必須進行收購才能進入系統設計領域，因為製造芯片是一回事，但將多個芯片連接在一起、適當冷卻、網絡連接、確保在該規模下的可靠性，這是一系列半導體公司沒有工程師能力解決的問題。"

"你認為NVIDIA在增量差異化方面投資最多的是什麼？"

"我認為NVIDIA在差異化方面主要關注供應鏈相關的事情。這可能聽起來像是'哦，他們只是在訂購東西'，不，不，不，你必須與供應鏈深度合作，開發下一代技術，這樣你才能在任何人之前將它推向市場。因為如果NVIDIA停滯不前，他們就會被吞噬。"

"你知道，就像Andy Grove說的'只有偏執狂才能生存'，Jensen可能是世界上最偏執的人。在LLM熱潮之前很多年，他就知道他最大的客戶都在開發AI芯片。在LLM熱潮之前，他的主要競爭對手還在說'我們應該製造GPU'，而他之所以能保持領先，是因為他能以其他人無法達到的規模將技術推向市場。無論是在網絡、光學、水冷，還是在供電方面，他都在推出其他人沒有的技術。他必須與供應鏈合作，教導這些供應鏈公司，當然他們也在幫助，他們有自己的能力來建造今天不存在的東西。而NVIDIA現在試圖每年都這樣做，這是令人難以置信的。"

"Blackwell、Blackwell Ultra、Ruben、Ruben Ultra，他們進展得如此之快，每年都在推動如此多的變革。當然人們會說'哦不，Blackwell有一些延遲'，但這是當然的，你看看他們對供應鏈施加了多大的壓力。這種年度節奏是競爭優勢的多大部分？因為通過這樣做，似乎幾乎阻止了競爭對手趕上，因為即使你滑到了Blackwell的位置，12個月內他們就已經在下一代了。"

"他已經在計劃提前兩到三年的事情了。有趣的是，NVIDIA的很多人會說Jensen不會計劃超過一年或一年半，因為他們會改變事情，並且能夠那麼快地部署。其他每個半導體公司都需要數年時間來部署，進行架構變更。"

"但你說如果他們停滯不前，就會面臨競爭，那麼他們的脆弱領域是什麼？市場上需要發生什麼，其他替代方案才能獲得更多工作負載份額？"

"對NVIDIA來說，主要是這樣的 - 這個工作負載規模很大，已經超過1000億美元的支出。對於最大的客戶來說，他們有多個客戶每年花費數十億美元。我可以雇用足夠多的工程師來弄清楚如何在其他硬件上運行我的模型。也許我現在還不能弄清楚如何在其他硬件上訓練，但我可以弄清楚如何在其他硬件上進行推理。所以NVIDIA在推理方面的軟件護城河實際上小得多，但他們在硬件方面的優勢更大，因為他們就是有最好的硬件。"

"最好的硬件是什麼意思？這意味著資本成本、運營成本和性能。性能TCO(總擁有成本)是關鍵。NVIDIA的整個護城河在於，如果他們停滯不前，他們的性能TCO就不會增長。但有趣的是，他們確實在增長。比如Blackwell，它不僅速度快得多，在非常大的模型推理方面快了10到15倍，因為他們針對大型語言模型進行了優化，他們還決定要降低一些利潤率，因為他們在與亞馬遜的芯片、TPU、AMD等競爭。"

"在所有這些因素之間，他們決定不能只是每兩年讓性能翻倍(摩爾定律)，而是決定每年要將性能TCO提高5倍。至少這是Blackwell的目標，我們會看看Ruben會如何。但在一年內實現5倍以上的性能TCO提升是一個瘋狂的速度。然後你再加上AI模型實際上在相同規模下變得更好，提供LLM的成本正在下降，這將刺激需求。"

"為了澄清你說的一點，或者至少重述一下以確保我理解 - 當你說軟件對訓練更重要時，你的意思是CUDA在訓練方面比在推理方面更具差異化作用，對嗎？"

"我認為投資界的很多人把CUDA(這只是NVIDIA所有軟件的一層)稱為NVIDIA的全部軟件。實際上有很多層的軟件，但為了簡單起見，無論是運行在交換機上的，還是用於艦隊管理的，所有NVIDIA製造的這些軟件，我們就簡單稱為CUDA。"

"所有這些軟件都極其難以複製。事實上，除了大型科技公司外，沒有其他人有部署能力。A100 GPU的規模就像是微軟的推理集群，而不是訓練集群。所以當你談論這裡的難度時，在訓練方面，這是用戶不斷實驗，研究人員說'讓我們試試這個，試試那個'，我沒有時間去優化和手動調整性能，我依賴NVIDIA的性能在現有軟件堆棧下或很少努力就能達到很好的效果。"

"但當我進行推理時，微軟正在部署5-6個模型，這些模型產生了多少億的收入？所有OpenAI的收入加起來可能有100億美元的推理收入。他們只有10億收入，但部署了5個模型 - GPT-4、40、40 mini，現在還有推理模型，01等。所以他們部署的模型很少，這些模型每六個月才改變一次。每六個月他們得到一個新模型並部署它，在這個時間框架內，你可以手動優化性能。"

"因此微軟已經在其他競爭對手的硬件上部署了GPT風格的模型，比如AMD。他們也用一些自己的硬件，但主要是AMD。他們可以通過軟件來優化這個，因為他們可以投入數百名工程師，數十名工程師的時間，數百或數千個工程師小時來做這個工作，因為這是一個如此統一的工作負載。"

"我想讓你評論一下這個圖表。這是我們今年早些時候展示的一個圖表，我認為這對我來說是一個重要時刻，當時Jensen在中東時首次表示，在未來四年內，我們不僅會有一萬億美元的新AI工作負載，而且還會有一萬億美元的CPU替換，數據中心替換工作負載。"

"這是一個嘗試建模的努力，當我們在播客中提到它時，他似乎表示這在方向上是正確的。他仍然相信，這不僅僅是關於預訓練，因為世界上有很多關於預訓練會不會繼續的爭論。這似乎表明有很多與預訓練無關的AI工作負載他們正在進行，但也表明他們有所有這些數據中心替換的工作。你相信這個嗎？我聽到很多人反駁說，人們不可能用一堆NVIDIA GPU重建CPU數據中心，這完全沒有意義，但他的論點是，越來越多的應用程序，甚至像Excel和PowerPoint這樣的東西，都在變成機器學習應用，需要加速計算。"

"NVIDIA長期以來一直在推動非AI工作負載使用加速器。專業視覺化就是一個例子，Pixar在製作每部電影時都使用大量GPU。所有這些模擬工程應用程序都確實使用GPU。但我要說的是，與AI相比，這些只是九牛一毛。"

"另一個我認為與你的圖表有些爭議的方面是，IBM大型機每個週期的銷售量和收入都在增長。是的，灣區沒有人使用或談論大型機，但它們仍在增長。我認為同樣的道理也適用於CPU，適用於傳統工作負載。就因為AI來了，並不意味著Web服務會放緩或數據庫會放緩。"

"現在發生的是，那條線是這樣的，而AI的線是這樣的。此外，當你談到這些應用程序現在都是AI時，比如帶有Copilot的Excel或Word，它們仍然會保留所有那些傳統操作。你不會去掉以前有的東西。西南航空不會停止預訂航班，他們只是在航班之上運行AI分析，也許是為了更好地定價或其他目的。"

"我要說的是這種情況仍然存在，但有一個被誤解的替換因素。考慮到人們部署的規模，數據中心供應鏈的緊張程度 - 不幸的是，數據中心需要更長的供應鏈時間。這就是為什麼你會看到像Elon這樣的做法。那麼我如何獲得電力呢？你可以像Coreweave那樣，去找加密貨幣礦業公司，直接清空他們的場地，放入大量GPU，就像他們在德克薩斯州做的那樣，改造數據中心，放入GPU。"

"或者你可以像其他一些公司做的那樣 - 為什麼我的CPU服務器的折舊期從三年延長到了六年？因為Intel的進展一直是這樣的。所以實際上，舊的Intel CPU並沒有好到哪裡去，但突然在過去幾年，AMD突然崛起，ARM CPU突然崛起，Intel開始扭轉局面。"

"我現在可以升級 - 亞馬遜數據中心中最多的CPU是2015年到2020年間製造的24核Intel CPU，基本上是相同的架構。這個24核CPU，我現在可以買128核或192核的CPU，每個CPU核心的性能都更高。如果我用一台服務器替換六台，我基本上就憑空創造了電力，對吧？因為這些超過六年的舊服務器可以報廢了，用新服務器的資本支出，我可以替換這些舊服務器，現在每次我這樣做，我就可以再放入一台AI服務器。"

"所以這就是某種程度的替換。我仍然需要更多的總容量，但這個總容量可以由更少的機器提供，如果我買新的話。一般來說，市場不會萎縮，它仍然會增長，只是遠不及AI的增長。而AI正在導致這種行為 - 我需要替換以獲得電力。"

"這讓我想起Sacha上週在播客上說的一點，我看到很多人重播並且我認為被相當誤解了。他上週在播客上說他受到電力和數據中心的限制，而不是芯片的限制。我認為這更多是對真正瓶頸的評估，也就是數據中心和電力，而不是GPU，因為GPU已經上線了。我認為你剛才的論述有助於澄清這一點。"

"在我們深入討論NVIDIA的替代方案之前，我想我們應該談談你最近文章中提到的預訓練擴展爭議。Ilia是提出這個問題的最可信的AI專家，然後這個觀點被反覆討論和交叉分析。Bill，重複一下這個觀點是什麼 - 我認為Ilia說數據是AI的化石燃料，我們已經消耗了所有的化石燃料，因為我們只有一個互聯網。所以我們從預訓練獲得的巨大收益不會重複出現。一些專家一兩年前就預測數據會用完，所以這個論點的出現並不是憑空而來。無論如何，讓我們聽聽Dylan怎麼說。"

"預訓練擴展定律其實很簡單。你獲得更多的計算能力，然後我把它投入到模型中，它就會變得更好。這實際上分為兩個軸：數據和參數。你知道，模型越大，數據越多越好，而且實際上有一個最佳比率。Google發表了一篇名為'Chinchilla'的論文，說明了數據與參數(模型大小)的最佳比率，這就是擴展的問題。"

"當數據用完時會發生什麼？我基本上不會獲得更多數據，但我繼續增加模型的大小，因為我的計算預算在不斷增長。不過這有點不公平，因為我們幾乎還沒有開始利用視頻數據。還有大量未被利用的數據，只是視頻數據比書面數據包含更多信息，所以你現在放棄了這些。但我認為這有點誤解，更重要的是，文本是最有效的領域。人們通常說一張圖片勝過千言萬語，但如果我寫100個字，你可能就能理解..."

"而且大多數這些視頻的文字記錄已經存在了。"

"是的，許多視頻的文字記錄已經包含在內了。但無論如何，數據是一個重要的軸心。現在問題是，這只是預訓練而已。所謂預訓練一個模型，不僅僅是預訓練本身，還有很多其他元素。人們一直在談論推理時間的計算很重要，如果你弄清楚如何讓模型思考和遞歸，就像'哦，這不對，讓我換個方式思考，哦，這也不對'，就像你不會雇用一個實習生或博士說'嘿，X的答案是什麼'，而是說'去研究這個問題'，然後他們會回來給你答案。"

"推理時間的計算很重要，但更重要的是，隨著我們繼續獲得更多計算能力，如果數據用完了，我們能否改進模型？答案是我們幾乎可以憑空創造數據，至少在某些領域是這樣。這就是整個擴展定律的爭論 - 我們如何創造數據？"

"那麼Ilia的公司最可能在做什麼？Mir的公司(Mir Madi是OpenAI的CTO)最可能在做什麼？所有這些公司，包括OpenAI，都在關注什麼？他們有Noah Brown，他是推理領域的重要人物，正在各地做路演演講。他們在做什麼？他們在說，我們仍然可以改進這些模型。是的，在推理時間花費計算很重要，但我們在訓練時間做什麼呢？因為你不能只告訴一個模型'多思考'它就會變得更好，你必須做大量的訓練。"

"這就是我拿模型和目標函數的過程。比如說，81的平方根是多少？如果我問很多人81的平方根是多少，很多人能回答，但我打賭如果給他們更多時間思考，會有更多人能回答出來。這可能是個簡單的例子，但你讓現有模型去做這件事，讓它運行每個可能的排列組合，不是所有可能的，而是從5開始，然後每當它不確定時就分支成多個路徑。你開始時有數百個所謂的'展開'或生成的數據軌跡，這些大部分是垃圾，你把它縮減到只保留那些得到正確答案的路徑。好的，現在我把這個作為新的訓練數據輸入。"

"我在每個可以進行功能驗證的領域都這樣做。功能驗證，也就是說'嘿，這段代碼能編譯'，'嘿，我的代碼庫中的這個單元測試'，我如何生成解決方案，如何生成函數？現在你一遍又一遍地在許多不同的領域做這件事，在那些你可以功能性地證明它是真實的領域。你生成所有這些數據，扔掉絕大多數，但現在你有了一些思維鏈可以訓練模型，然後它會學會更有效地做這件事，而且它會泛化到其他領域。"

"這就是現在的整個領域。當你談論擴展定律時，收益遞減點還沒有被證實。因為它更多的是說，擴展定律是一個對數軸，也就是說需要投入10倍的資金才能得到下一次迭代。好吧，從3000萬到3億，從3億到30億是相關的，但當Sam想從30億到300億時，籌集這筆錢就有點困難了。這就是為什麼最近的融資輪次有點像'哦糟糕，我們不能在下一輪花300億'。"

"所以問題是，好吧，這只是一個軸心，我們在合成數據方面走到哪裡了？我們還處於非常早期的階段。我們在合成數據上可能只花了幾千萬美元。"

"在談到合成數據時，你用了'在某些領域'這個限定語，當他們發布01時也有類似的限定語。我只是說這兩個擴展在某些領域表現更好，在其他領域則不那麼適用，我們需要弄清楚這一點。"

"我認為AI一個有趣的地方是，在2022-2023年，隨著擴散模型和文本模型的發布，人們說'哇，藝術家是最倒霉的，而不是技術工作'。實際上，這些東西在技術工作方面表現很差。但是通過這個新的合成數據軸心和實際的測試時間計算，在我們可以教導模型的領域 - 我們不能教它什麼是好的藝術，因為我們沒有辦法功能性地證明什麼是好的藝術 - 我們可以教它寫出很好的軟件，我們可以教它如何做數學證明，我們可以教它如何設計系統，因為雖然有權衡取捨，這不是一個簡單的是非問題，特別是在工程系統方面，但這是你可以功能性地驗證的東西 - 這個是否有效，或者這是否正確，然後模型可以更頻繁地迭代。"

"這讓我想起AlphaGo的事情，為什麼它是一個可以允許新穎走法和下法的沙盒，因為你可以遍歷它，讓它不斷創造和創造。"

"戴上我的公共投資者帽子，世界上對2025年的NVIDIA前景存在很多緊張情緒，這個預訓練的問題。如果事實上我們已經從預訓練中獲得了90%的低垂果實，那麼人們真的需要購買更大的集群嗎？我認為世界上有一種觀點，特別是在Ilia的評論之後，認為預訓練90%的好處已經消失了。但然後我看到本週來自HTan的評論，在他們的財報電話會議上說所有超大規模公司都在建設這些百萬XPU集群。"

"我看到來自X的評論說他們要建設20萬或30萬GPU集群，Meta據報道也在建設更大的集群，Microsoft在建設更大的集群。你如何調和這兩件事？如果每個人都是對的，預訓練已經結束了，那為什麼每個人都在建設更大的集群？"

"擴展回到什麼是最佳比率，我們如何繼續增長？當我們沒有更多數據時，或者數據很難獲得時(比如視頻數據)，僅僅盲目增加參數數量不會帶來那麼多收益。然後還有一個問題是這是一個對數圖表，你需要投入10倍才能獲得下一個跳躍，對吧？所以當你看這兩個問題時，'哦糟糕，我需要投資10倍'，而且因為我沒有數據，我可能無法獲得全部收益，但數據生成這一面我們還處於非常早期階段。"

"所以關鍵是，我仍然會獲得足夠的收益，使其成為正回報，特別是當你看競爭動態時 - 我們的模型與競爭對手的模型相比。所以從10萬增加到20萬或30萬是一個理性的決定，即使你知道預訓練的一次性大收益已經過去了，或者說要獲得這種收益的成本呈指數級增長。"

"對的，所以收益仍然存在，但像整個'Orion失敗'的說法圍繞著OpenAI的模型，他們沒有發布Orion而是發布了01，這是一個不同的軸心。部分原因是因為這些數據問題，但部分原因是他們沒有擴大10倍，因為從4擴大10倍實際上是... 我認為這是Gavin的觀點。"

"我認為這變得有爭議的原因之一是，至少據我所知，Dario和Sam在此之前給人的印象是他們只需要建立下一個最大的東西就能獲得同樣的收益。所以當我們到達這個階段，情況並不完全是這樣，人們就會說'這意味著什麼？'這讓人們開始抬頭思考。"

"我認為他們從來沒有說過Chinchilla擴展定律會給我們帶來AGI(通用人工智能)。他們說擴展需要大量的計算，如果你必須生成大量數據並扔掉大部分，因為只有一些路徑是好的，那麼你在訓練時就會花費大量計算。這就是那個有點像，我們在接下來的六個月到一年內可能會看到模型改進得比去年更快的軸心，因為有這個新的合成數據生成軸心，而我們能投入的計算量，我們仍然在擴展定律的這裡，而不是這裡，我們還沒有在合成數據生成、功能驗證、推理訓練上花費數十億美元，我們只花了幾千萬美元。"

"那麼當我們擴大這個規模會發生什麼？當然還有推理時間計算，也就是在推理時花時間來獲得更好的結果。所以這是可能的，事實上這些實驗室的許多人都相信，由於通過新方法解鎖了這個新軸心，下一年或下六個月的收益會更快。它仍然需要規模，因為這需要巨大的計算量，你生成的數據比網絡上存在的還要多，然後你扔掉大部分，但你必須不斷運行模型。"

"你認為哪些領域最適合這種方法？合成數據在哪裡最有效，也許你能舉個例子，一個會很好的場景和一個不會那麼好的場景？"

"是的，這回到了我們能在功能上驗證什麼是真實的還是不真實的這一點。在大學裡你上什麼課，拿到卡片回來時，你會說'這是胡說八道'或者'天啊，我搞砸了'，有些課程你可以確定性地給出成績。"

"所以如果能夠功能性地驗證，那就太棒了。如果必須經過判斷...有兩種方式來判斷輸出，一種是不使用人類，這有點像Scale AI最初在做什麼，他們使用大量人力來創建好的數據，標記數據。但對於這個級別的數據，人類無法擴展，人類每天在互聯網上發帖，我們已經差不多用完了這些數據。"

"那麼哪些領域有效呢？在Google中，當他們將數據推送到任何服務時，都有大量單元測試，這些單元測試確保一切運作正常。為什麼我不能讓LLM生成大量輸出，然後使用這些單元測試來評分這些輸出呢？因為它是通過或失敗，不是主觀的。然後你也可以用其他方式評分這些輸出，比如運行時間的長短。"

"還有其他領域，比如圖像生成，實際上很難說哪張圖片對你來說更美，對我來說更美。你可能喜歡日落和花朵，而我可能喜歡海灘，你無法真正爭論什麼是好的。所以那裡沒有功能性驗證，只有主觀性。"

"所以這種客觀性質在哪裡我們有客觀評分？我們在代碼中有，在數學中有，在工程中有。雖然這些可能很複雜，比如工程不僅僅是'這是最好的解決方案'，而是'考慮到我們擁有的所有資源和所有這些權衡，我們認為這是最好的權衡'。這通常就是工程的結果。但我仍然可以看所有這些軸心，而在主觀的事情上，比如'寫這封郵件的最好方式是什麼'或'與這個人談判的最好方式是什麼'，這就很困難了，這不是客觀的東西。"

"你從超大規模公司那裡聽到什麼？我的意思是，他們都在說我們明年的資本支出會增加，我們正在建設更大的集群。這實際上在發生嗎？外面到底發生了什麼？"

"是的，所以我認為當你看街頭對資本支出的估計時，它們都太低了。你知道，基於幾個因素。當我們追蹤世界上每個數據中心時，特別是Microsoft，現在的Meta和Amazon，還有很多其他公司，但這些公司特別在數據中心容量上的支出是瘋狂的。而且當這些電力上線時，這是很容易追蹤的。"

如果你查看所有不同的監管文件、衛星圖像以及我們所做的所有分析，你就能看出他們將會有多少數據中心容量。這個趨勢正在加速，那麼這些空間要用來做什麼呢？事實證明，要填滿這些空間，你可以根據每個GPU的總功耗做一些估算。雖然薩查(可能指NVIDIA CEO黃仁勳)說他要放緩一點，但他們已經簽署了明年的租約。他說部分原因是他預計明年上半年的雲端營收會加速增長，因為他們將有更多的數據中心容量，而目前正受到容量限制。

所以回到"擴展是否已死"這個問題 - 為什麼Mark Zuckerberg要在路易斯安那州建造一個2吉瓦的數據中心？為什麼亞馬遜要建造這些多吉瓦級的數據中心？為什麼谷歌和微軟要建造多個吉瓦級的數據中心，還要花費數十億美元購買光纖來連接它們？因為他們認為需要在規模上取勝，所以要用超高帶寬將所有數據中心連接在一起，讓它們能像一個數據中心一樣運作，朝向一個目標前進。

所以當你看到那些最了解情況的人在投資什麼時，這個"擴展已結束"的說法就不攻自破了。你在開始時談到了NVIDIA在這些用於預訓練的大型連貫集群方面的差異化優勢，你能看到什麼嗎？我猜有人可能對推理非常看好並繼續建設數據中心，但他們可能原本打算從10萬個節點增加到20萬個再到40萬個，如果這個預訓練趨勢是真實的，他們現在可能不會這麼做了。你看到任何能讓你了解這個維度的跡象嗎？

當你思考訓練神經網絡時，它包含前向傳播和反向傳播。前向傳播基本上是生成數據，其計算量是反向傳播（用於更新權重）的一半。當你看這種新的範式 - 合成數據生成、評分輸出，然後訓練模型時，你需要進行多次前向傳播才能進行一次反向傳播。而為用戶提供服務時也只是前向傳播。

事實證明，在訓練過程中有大量的推理計算。實際上，訓練中的推理計算量比更新模型權重的計算量還要大，因為你需要生成數百種可能性，但最終只在其中幾個上進行訓練。所以這個範式非常重要。

另一個相關的範式是，當你在訓練模型時，是否每個方面都需要在同一位置進行？答案是取決於你在做什麼。如果你在預訓練階段，那麼也許不需要，但是你確實需要所有東西都在一個地方。這就是為什麼微軟在第一季度和第二季度簽署了這些大規模的光纖合約。為什麼他們要在威斯康星州、亞特蘭大、德克薩斯州和亞利桑那州等地建設多個相似規模的數據中心？因為他們已經看到了研究顯示可以更適當地分割工作負載 - 這個數據中心不是服務用戶，而是運行推理，只是運行推理然後丟棄大部分輸出，因為只有部分輸出是好的，因為他們在對其進行評分。

當他們在其他地方更新模型時，他們同時也在做這些工作。整個預訓練的範式並不是在衰退，只是每一代、每一次增量改進都呈對數級增長的成本。人們正在尋找其他方法，而不是僅僅繼續這種模式。事實上，通過這種推理和推理的方式，我可以在不需要增加對數級支出的情況下，實現下一代的改進。

實際上我要兩種方法都用，因為每次模型的突破都帶來了巨大的價值。我覺得很有趣的是，今天早上我聽到CNBC的克萊默在談論，這是否會重演2000年的思科情況。週日晚上我在奧馬哈和比爾共進晚餐，他們顯然是公用事業的大投資者，他們正在關注數據中心建設的情況，他們也在問這是否會重演2000年的思科。

所以我讓團隊調出2000年思科的圖表，你知道，他們的市盈率峰值在120倍左右。如果你看收入和營業收入的下跌，以及市盈率壓縮了70%，從120倍降到接近30倍。所以在晚餐談話中我說，英偉達現在的市盈率是30倍，不是120倍。要重演思科那樣的情況，你得認為市盈率會從這裡再壓縮70%，或者他們的收入會下降70%，或者他們的盈利會下降70%。

我們都對那段歷史有創傷後應激障礙，我也經歷過那個時期，沒人想重演那樣的情況。但當人們做這種比較時，在我看來是缺乏見識的。這並不是說不會有回調，但考慮到你剛才告訴我們的明年的建設計劃，以及你告訴我們的擴展規律仍在繼續，你對人們拿英偉達與思科相比較有什麼看法？

這裡有幾個不公平的比較。思科的收入很大一部分是通過私人信貸投資來建設電信基礎設施。而當我們看英偉達的收入來源時，很少依賴私人信貸。在某些情況下，確實有私人信貸，比如Coreweave，但Coreweave實際上是由微軟支持的。資金來源有很大的不同。另外，即使考慮通貨膨脹因素，當時進入這個領域的私人資本遠大於現在。

雖然人們說風險投資市場正在瘋狂地給這些公司估值，但就在節目開始前我們還在討論這個問題 - 風險市場和私人市場甚至還沒有真正參與進來。中東和這些主權財富基金的私人市場資金幾乎還沒有進入，為什麼他們不會增加更多投資呢？

所以資金來源有很大的區別 - 現在是人類歷史上最賺錢的公司的正現金流，而不是投機性的信貸支出。我認為這是一個重要方面，也給了它一定的穩定性。這些盈利的公司會更理性一些。我認為企業界對AI的投資比互聯網浪潮時期更有信念和更大規模。

也許我們可以轉換一下話題，你剛才多次提到了推理時間推理，這顯然是提升智能的一個新向量，我最近讀到你的一些分析，說推理時間推理比單純的預訓練要消耗更多計算資源。你能否為我們解釋一下，從計算消耗的角度來看，推理時間推理是什麼，為什麼它如此計算密集？這導致的結論是，如果這確實將繼續作為智能的新向量擴展，看起來它會比之前更加計算密集。

預訓練可能正在放緩或變得太昂貴，但還有這些合成數據生成和推理時間計算的其他方面。推理時間計算表面上聽起來很棒 - 我不需要花更多錢來訓練模型，但仔細想想，這實際上不是你想要的擴展方式，你這樣做只是因為不得不這樣做。

想想看，GPT-4的訓練花費了數億美元，但它正在產生數十億美元的收入。所以當你說微軟的資本支出很瘋狂時，但他們在GPT-4上的支出相對於他們獲得的投資回報來說是很合理的。

現在當你說想要下一個突破時，如果我只需要投入大量資金來訓練一個更好的模型，那很好。但如果我不需要提前投入大量資金，而是在產生收入時部署這個更好的模型，而不是在訓練模型時提前投入，這聽起來也很棒。但這帶來了一個很大的權衡。

當你運行推理時，你讓模型生成很多內容，但答案只是其中的一部分。今天當你打開ChatGPT使用GPT-4時，你說些什麼，得到回應，發送內容，得到回應，不管是什麼。所有生成的內容都會發送給你。現在你有了這個推理階段，OpenAI不想展示給你，但一些開源的中國模型，比如阿里巴巴和DeepSeek，他們發布了一些開源模型，雖然不如OpenAI那麼好，但如果你想看的話，他們會向你展示推理是什麼樣的。

OpenAI也有一些例子，它會生成大量內容，有時會在中文和英文之間切換，不管是什麼，它都在思考 - 它在想"我應該這樣做嗎？我應該把它分解成這些步驟嗎？"然後才得出答案。表面上看很棒，我不需要在研發或資金上投入更多。說得籠統一點，微軟在財務上不會把訓練模型當作研發，但他們不需要提前進行這種研發，你是在使用時才付費。

但想想這意味著什麼。比如說，我們做了很多測試，其中一個簡單的例子是"幫我生成這段代碼"。我用幾百個字描述這個函數，我得到一個回應，很好。當我使用Claude或任何其他推理模型時，我發送相同的請求，幾百個token，我得到差不多的回應，大約一千個token，但在中間過程中產生了一萬個token的思考過程。

這一萬個思考token實際意味著什麼？意味著模型產生了10倍的token。如果微軟在推理上產生了比如說100億美元的收入，他們在這方面的利潤率很好，他們已經說明了這一點，大約在50%到70%之間，取決於你如何計算OpenAI的利潤分成。對100億美元的收入來說，他們的成本是幾十億美元。

顯然，更好的模型可以收取更高的費用，所以Claude確實收費更多，但現在你的成本從"我輸出一千個token"增加到"我輸出一萬一千個token"，我的支出增加了10倍，但產出的並不是相同的東西 - 雖然質量更高。

這還只是其中一部分，這種簡單的描述具有欺騙性。不僅僅是10倍，因為如果你去看Claude，儘管它使用與GPT-4相同的模型架構，但每個token的成本也顯著更高，這是因為我們正在看的這張圖表。

這張圖表顯示了GPT-4的情況，如果我生成比如說一千個token，在圖表底部的是GPT-4或Llama 405B（這是開源模型所以更容易模擬準確指標）。如果我要保持用戶體驗的穩定，也就是說他們獲得的token數量和速度保持不變，當我問一個問題時，它生成單位，生成代碼，不管是什麼，我可以將多個用戶請求組合在一起，對於Llama 405B，我可以在一台NVIDIA服務器（大約30萬美元的服務器）上同時處理256個用戶請求。

但當我使用Claude時，因為它在進行那個一萬token的思考階段 - 這基本上就是整個上下文長度的問題，上下文長度不是免費的。上下文長度或序列長度意味著它必須計算注意力機制，也就是說它在生成這個KV緩存並持續讀取這個KV緩存時會消耗大量內存。

現在最大的批處理大小，也就是可以同時服務的用戶數量，只有原來的1/4到1/5。所以不僅我需要生成10倍的token，每個生成的token能同時服務的用戶數也減少了4到5倍。當你考慮到單個用戶生成單個token的成本增加了4到5倍，再加上我要生成10倍的token，你可以說Claude這樣的模型在輸入輸出上的成本增加了50倍。我知道原始Claude發布時是10倍的計算單元，但在對數刻度上，我不知道是10倍，而且你還需要有更多的計算資源來服務相同數量的客戶。

這裡有好消息也有壞消息，布拉德。如果你只是在賣NVIDIA硬件，而且它們保持這種架構，這就是我們的擴展路徑，你將會消耗更多的硬件。但對於在另一端生成內容的人來說，除非他們能把成本轉嫁給最終消費者，否則他們的利潤率會被壓縮。

關鍵是你確實可以把成本轉嫁給最終消費者，因為這不僅僅是在某個基準測試上好了多少百分比，而是它之前完全做不到，現在可以做到了。他們現在正在進行一個測試，向最終消費者收取10倍的費用，而且是每個token收取10倍。記住，他們還要為10倍的token付費，所以實際上消費者每次查詢要付50倍的費用。

但他們從中獲得了價值，因為現在它突然可以通過某些基準測試，比如SBench（軟件工程基準測試），這只是一個生成像樣代碼的基準測試。想想前端網頁開發，你付給前端開發者多少錢，付給後端開發者多少錢，相比之下，如果他們使用Claude，能輸出多少代碼，能產出多少？是的，查詢很貴，但比起人工成本來說還是便宜得多。

每一個生產力的提升，每一個能力的躍進，都開啟了一個全新的任務類別，因此我可以為此收費。這就是整個軸心 - 是的，我為獲得相同的輸出花費了更多，但你用這個模型獲得的並不是相同的輸出。

我們是否低估或高估了企業級對Claude模型的需求？你聽到了什麼？我要說Claude這樣的模型還處於非常早期的階段，人們甚至還沒有真正理解它。Claude剛剛破解了這個難題並在實施，但你猜怎麼著？現在在一些匿名基準測試中，比如叫做LLM Cy的地方，這是不同LLM競爭和人們對它們投票的競技場，有一個Google的模型正在進行推理，雖然還沒有發布，但很快就會發布。Anthropic也將發布一個推理模型，這些公司會相互超越。

而且他們在推理方面的訓練計算投入還很少，但他們看到了投入更多計算的明確路徑，也就是在擴展法則上更進一步。比如說，我只花了1000萬美元，那麼這意味著我可以在擴展上跳躍2到3個對數級別，因為我已經有了計算能力。我可以在推理方面從1000萬快速躍升到10億，再到100億，在某些有功能驗證器的基準測試中，在未來6個月到1年內，我們將從這些模型中獲得巨大的性能提升。

快速問一個問題，我們承諾要談到這些替代方案，所以最終要談到，但如果回顧互聯網浪潮，當所有風險投資公司剛開始時，他們都在使用Oracle和Sun，但5年後他們既不用Oracle也不用Sun了。有人認為它從開發沙盒世界轉變為優化世界。這種情況是否可能再次發生？這裡是否有相似性？如果可以的話，請談談為什麼後端如此陡峭且便宜，就像你知道的，如果你使用稍微落後一代的模型，或者你可以節省的token價格是驚人的。

是的，今天Claude的成本非常高，你降級到GPT-4就便宜很多，再降到GPT-4 Mini就更便宜了。為什麼？因為現在使用GPT-4 Mini，我要與Llama競爭，要與DeepSeek競爭，要與Mistral競爭，要與阿里巴巴競爭，還要與其他許多競爭者競爭。

你認為這些是市場出清價格嗎？我認為，此外還有一個問題是推理小型模型相當容易。我可以在一個AMD GPU上運行Llama 70B，可以在一個NVIDIA GPU上運行Llama 70B，很快也可以在亞馬遜的新Trainium上運行。我可以在單個芯片上運行這個模型，這是一個很容易...我不會說很容易，但比運行這種複雜的推理或非常大的模型要容易得多的問題。

所以存在這種差異。還有一個事實是，現在有15家不同的公司在Llama、阿里巴巴、DeepSeek和Mistral等不同模型上提供API推理服務。你在談論Cerebras、Groq和Fireworks等所有這些公司。是的，Fireworks等等。所有這些不使用自己硬件的公司。當然，Groq和Cohere正在使用自己的硬件來做這件事，但這裡的市場利潤很差。

去年年底，當MosaicML發布他們的Mixtral模型時，我們經歷了整個推理競爭到底部的過程，因為它在開源領域提供了前所未有的性能水平，導致價格迅速下降。因為每個人都在競爭API服務，作為API提供商，我能提供什麼讓你不從我這裡轉到他那裡？因為基本上是可替代的，我仍然在相同的模型上獲得相同的token。

所以這些公司的利潤率要低得多。微軟在開源模型上賺取50-70%的毛利率，這還包括他們必須分享的利潤。同樣，Anthropic在他們最近一輪融資中也展示了70%的毛利率，但那是因為他們有這個模型。你降級到這裡，很少有人使用這個來自OpenAI或Anthropic的模型，因為他們可以直接從Meta取得Llama的權重放在自己的服務器上，或者反過來使用眾多競爭性的API提供商，其中一些是風險投資支持的，一些是...錢。

所以這裡有所有這些競爭。不僅僅是說我退後一步，這是一個更容易的問題，而且如果模型小10倍，運行成本就便宜15倍。最重要的是，我還去掉了那個毛利率，所以它不是運行成本便宜15倍，而是便宜30倍。

這就是美妙之處 - 一切都是商品化的嗎？不是，但如果你在部署服務，這對你來說是很好的。第二，如果你是實驗室之一，你必須擁有最好的模型，否則你就什麼都不是。所以你看到很多試圖建立最好模型但失敗的公司在掙扎。可以說，你不僅要有最好的模型，實際上還必須有願意為最好模型付費的企業或消費者，因為最終，最好的模型意味著有人願意支付這些高利潤率。

所以我認為你很快就會發現，在模型方面，只有少數幾家公司能夠在這個市場上競爭。是的，我認為在誰願意為這些模型付費方面，我認為會有更多人願意為最好的模型付費。

第三個原因是Google內部使用TPU更有利可圖。順便說一下，微軟實際上租用的GPU很少。他們從內部工作負載或用於推理獲得的利潤更高，因為銷售tokens的毛利率在50-70%，而出租GPU服務器的毛利率低於這個數字。雖然這也是不錯的毛利率，但在他們引述的100億美元收入中，沒有任何部分來自外部GPU租賃。

如果Gemini變得極具競爭力，那麼第三方是否會間接使用Google的TPU？是的，絕對會。廣告、搜索、Gemini應用程序，所有這些都使用TPU。你上傳的每個YouTube視頻都會經過TPU處理(當然也會經過他們為YouTube製作的其他定制晶片)。有很多服務都會接觸到TPU，但你永遠不會直接租用它。因此，在租用市場中，據我所知只有一家公司佔據了Google TPU收入的70%以上，那就是蘋果。關於為什麼蘋果討厭英偉達，這可能是另一個故事了。

你剛剛深入分析了Trainium，能否像剛才談Google那樣談談亞馬遜的情況？有趣的是，亞馬遜的晶片我稱之為"亞馬遜基礎版TPU"。之所以這麼說，是因為雖然它使用更多矽晶片、更多記憶體，網路性能也與TPU相當(是4x4x4的環形拓撲)，但他們的實現方式效率較低。

例如，他們在主動式纜線上花費更多，因為他們與Marvell和Al chip合作開發自己的晶片，而不是與網路領導者博通合作(後者可以使用被動式纜線，因為他們的序列器性能很強)。他們的序列器速度較慢，使用更多晶片面積 - Trainium有很多這樣的特點，如果是作為商用晶片可能會很糟糕，但因為是亞馬遜自用就沒問題了。他們不用支付博通的利潤率，支付的是較低的利潤率。他們在HBM上支付給Marvell的利潤率也較低。他們採取各種措施來降低成本，使得他們的"亞馬遜基礎版TPU" Trainium 2在終端客戶和自身使用方面都非常具有成本效益，特別是在每美元的HBM容量和記憶體頻寬方面。

它有64的世界規模，但亞馬遜無法在一個機架中實現，需要兩個機架。每個晶片之間的頻寬比英偉達的機架慢得多，每個晶片的記憶體容量和頻寬也較低。但你不用支付每個晶片3萬到4萬美元，而是顯著更低，大約5千美元。這個差距太大了，亞馬遜將這個優勢轉嫁給客戶。

正是因為這個原因，亞馬遜和Anthropic決定建造一個擁有40萬個Trainium晶片的超級電腦。回到擴展定律是否已死的問題 - 他們建造40萬個晶片的系統是因為他們真的相信這個方向。在一個位置放置40萬個晶片對於服務推理來說並不實用，它更適合用來製作更好的模型。推理需要更分散的部署。這是一個巨大的投資。

雖然從技術角度來看並不是特別令人印象深刻(我剛才略過了一些確實令人印象深刻的方面)，但由於成本效益如此之高，我認為這對亞馬遜來說是一個不錯的策略。

讓我們轉向2025年和2026年的展望。過去30天裡，我們看到博通股價大漲，英偉達則有所回落，兩者差距約40%。博通被視為定制ASIC的投資標的，而人們開始質疑英偉達面臨的新競爭，預訓練的改進速度也不如從前。展望2025-2026年，你跟客戶討論什麼？哪些方面最容易被誤解？在你關注的領域中，最好的投資想法是什麼？

我認為有幾個要點：首先，博通確實贏得了多個定制ASIC訂單，不僅僅是Google。Meta正在擴大主要用於推薦系統的定制晶片，他們的定制晶片會變得更好。還有其他玩家，如正在製作晶片的OpenAI，以及蘋果(雖然不是完全與博通合作，但有一小部分會與博通合作)。他們現在有很多訂單，這些不會都在2025年實現，有些要到2026年。

由於是定制ASIC，可能會失敗或效果不好(就像微軟的情況)，因此永遠不會大規模生產;也可能像亞馬遜那樣性價比不錯，因此可以大規模生產。所以這裡確實存在風險。

但博通有兩個重要優勢：第一是定制ASIC業務，第二也很重要的是網路業務。是的，英偉達確實在賣很多網路設備，但當人們製作自己的ASIC時會怎麼做？他們可以選擇去亞馬遜，也可以選擇博通，或者去Marvell或其他競爭對手如Al chip等。博通非常有優勢可以製造NVSwitch的競爭產品，而NVSwitch被認為是英偉達在硬體方面相對於其他競爭對手最大的優勢之一。博通正在開發這個競爭產品，並會向市場供應。不只是AMD會使用這個NVSwitch的競爭產品，很多公司都會使用，因為他們自己沒有這個技術能力，所以要找博通來製造。

幫我們分析一下目前的半導體市場。你有ARM、博通、英偉達、AMD等，展望2025和2026年，整個市場是否會繼續上揚？從目前的水平來看，誰最有優勢？誰被高估了？誰被低估了？

我長期看好博通，但在接下來的六個月內，由於Google沒有數據中心空間，TPU採購會有所放緩。他們想要更多，但確實沒有地方放置。我們可以看到會有一個暫停期，但人們可能會忽視這一點。

除此之外，關鍵問題是誰會贏得未來的定制ASIC訂單？是Marvell還是博通會贏得未來幾代產品？這些新一代產品規模會有多大？超大規模雲端服務商是否能夠內部化更多這方面的業務？比如說，Google試圖擺脫博通已經不是秘密，他們可能成功也可能失敗。

不只是博通，談到英偉達和其他所有公司 - 我們經歷了兩年的大好形勢，2025年會是整合的一年嗎？還是這個行業會繼續表現良好？

我認為超大規模雲端服務商明年的支出計劃相當明確 - 他們會花費更多。因此，整個生態系統，包括網路設備商、ASIC供應商、系統供應商都會表現良好，無論是英偉達、Marvell、博通還是AMD，雖然有些會比其他表現更好。

人們真正應該關注的是2026年 - 這樣的支出是否會持續？英偉達明年的增長率會非常驚人，這會帶動整個供應鏈上升，帶動很多公司一起成長。但2026年才是關鍵時刻。

問題在於人們是否會繼續保持這樣的支出水平，這取決於模型是否會持續進步。如果模型停止進步，我認為會出現一個重大的清算事件。但這不會發生在明年。事實上，我認為明年模型會進步得更快。

另一個方面是新興雲服務商市場的整合。我們追蹤並與80家新興雲服務商交談，了解他們擁有多少GPU。問題是現在H100的租賃價格正在下跌，不僅是在這些新興雲服務商那裡。過去你必須簽4年合約並預付25%，簽訂一輪融資然後購買一個集群，僅此而已。現在你可以以比過去4年期合約更好的價格獲得3個月或6個月的合約。

不僅如此，亞馬遜的隨需GPU價格也在下跌。雖然相對來說仍然很貴，但價格正在快速下跌。80家新興雲服務商不可能都能存活，可能只有5到10家能活下來。因為其中5家是主權雲，另外5家是真正具有市場競爭力的。

這些可能無法生存的新興雲服務商佔整個AI產業收入的比例有多少？大致來說，超大規模雲端服務商佔收入的50-60%，其餘是新興雲/主權AI。因為企業購買GPU集群的數量仍然很低，對他們來說外包給新興雲服務商更好，只要能通過安全審查(某些公司如Coreweave就可以)。

2026年是否可能出現行業銷量或英偉達銷量比2025年下降的情況？當你看看即將推出的定制ASIC設計以及英偉達的晶片，每個晶片的內容都在爆炸性增長。製造Blackwell的成本是Hopper的兩倍以上，所以英偉達即使出貨量相同，仍然可以實現大幅增長。

那麼2026年是否可能出現行業收入下降或英偉達收入下降的情況？關鍵在於：模型是否會繼續快速進步，以及超大規模雲端服務商是否願意讓自由現金流降到零。順便說一下，我認為他們願意這樣做。我認為Meta和微軟可能會讓自由現金流接近零，然後繼續投資。但這只有在模型持續進步的情況下才會發生。

另一個問題是：我們是否會迎來來自中東、新加坡、北歐和加拿大養老基金等主權財富基金的大量資金注入？他們有能力寫很大的支票，雖然還沒有這樣做，但他們可以。如果情況繼續好轉，我真的相信OpenAI、xAI和Anthropic會繼續籌集更多資金，讓這個遊戲持續下去。

這不僅僅是關於OpenAI現在的收入有80億，明年可能會翻倍或更多。不，他們必須籌集更多資金來大幅增加支出，這樣才能保持引擎運轉。因為一旦其中一家增加支出，馬斯克的集群和他的計劃就迫使所有人都要增加支出，因為每個人都在想"我們不能讓馬斯克在規模上超過我們，我們必須投入更多"。所以這也是一場規模競賽，因為這是一個規模遊戲。

從帕斯卡賭注的角度來看，如果我投資不足，那就是最糟糕的情況。我可能會成為有史以來最賺錢業務中表現最差的人。但如果我投資過度，那麼股東會生氣，但沒關係，200億或500億美元，你可以從兩個角度來看待這個問題。

當然，如果這成為投資的理由，那麼投資過度的可能性就會上升。每個泡沫最終都會過度擴張。

對我來說，這不僅僅取決於模型的改進，更要回到SA上週告訴我們的話：最終一切都取決於購買GPU的人能產生多少收入。就像他上週說的，他每年會購買一定數量，這與他在當年或未來幾年能產生的收入有關。

所以他們不會遠超這些收入進行投資。他正在看著今年100億的收入，他知道這些推理收入的增長率，他和Amy正在預測他們能負擔多少支出。我認為扎克伯格也在做同樣的事，Sundar也是。如果假設他們是理性行事，那麼這不僅關係到模型的改進，還關係到底層企業使用他們服務的採用率，以及消費者的採用率和願意為使用ChatGPT、Claude或其他服務支付的費用。

所以，如果你認為基礎設施支出每年會增長30%，那麼你必須相信底層的推理收入(包括消費者端和企業端)也會在這個範圍內增長。

不過確實存在提前支出的因素，這是時點支出與未來5年服務器預期收入的關係。我完全同意，但模型進步的全部意義在於產生更多收入並得到部署。我同意這個觀點，但人們確實在超前支出。

很高興你能來到這裡。作為一個分析師同行，你們做了很多深入研究。恭喜你們業務的成功。我認為你為整個生態系統提供了很多重要信息。

Bill說到擔憂之牆時提到，有時候我們都在談論和尋找泡沫，這反而可能防止泡沫真正發生。但作為投資者和分析師，我看到確實有人在支出時沒有相應的收入支撐，正如你所說在超前支出。另一方面，我們上週聽Sacha說他有收入支撐，他說明了自己的收入情況，但我們還沒有聽到其他所有人這麼說。所以2025年看看誰能真正產生收入會很有趣。

我認為你已經可以看到一些較小的二三線模型正在改變商業模式，退出或不再參與軍備競賽。我認為這是創造性破壞過程的一部分。

很高興你能親自來到這裡。明年再見。

提醒所有人，這些只是我們的觀點，不構成投資建議。

