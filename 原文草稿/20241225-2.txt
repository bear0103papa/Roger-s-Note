if you look at all of the different uh regulatory filings and you satellite imagery all these things that we do you can see that hey they're going to have
this much data center capacity right right um what you it's accelerating what are you going to fill in there right it turns out you you have to fill to fill
it up you know you can make some estimates around how how much power is each GPU Allin everything right satcha said he's going to slow down that a little bit but they' signed deals for
next year rentals right for some in some of these cases right part of the reason he said is he expects his Cloud Revenue in the first half of next year to
accelerate because he said we're going to have a lot more data center capacity and we're currently capacity constraint so you know what they're you know like again going back to the is scaling dead
then why is Mark Zuckerberg building a 2 gwatt Data Center in Louisiana right why is why is Amazon building these multi- gigawatt data centers why is Google why
is Microsoft building multiple gigawatt data centers plus buying billions and billions of dollars of fiber to connect them together because they think hey I
need to win on scale so let me just connect all the data centers together with super high bandwidth so then I can make them act like one data center right
towards one job right so this is this this whole like is scaling over narrative F falls on its face when you
see what the people who know the best are spending on right you talked a lot at the beginning about nvidia's
differentiation around these large coherent clusters that are used in
pre-training um can you see anything like I guess one someone might be super bullish on inference and keep building
out a data but they might have thought they were going to go from 100,000 noes to 200 to
400 and might not be doing that right now if if this pre-training thing is
real are you seeing anything that gives you any visibility on that Dimension so um when you think about training a
neural network right it is it is doing a forwards pass and a backwards pass right forwards pass is generating the data basically um and it's half as much
compute as the backwards pass which is updating the weights when you look at this new paradigm of synthetic data
generation grading the outputs uh and then training the model you going to do many many many forward passes before you
do a backwards pass what is serving a user that's also just a forwards pass so it turns out that there is a lot of um
inference in training right in fact there's more inference in training than there is updating the model weights
because you have to generate hundreds of possibilities and then oh you only train on a couple of them right so there there
is that Paradigm is very relevant um the other Paradigm I would say that is very relevant is um when you're when you're
training a model um do you necessarily need to be collocated for every single aspect of it right right um and this is
and what's the answer the answer is depends on what you're doing if you're in the pre-training Paradigm then maybe you don't yeah you need it to be
collocated right um You need everything to be in one spot yeah why Microsoft in in q1 and Q2 sign these massive fiber
deals right um and why are they why are they building multiple similar size data centers in Wisconsin and Atlanta and and
and Texas and so on and so forth right in Arizona why are they doing that because they already see the researches there for being able to split the
workload more appropriately which is hey this data center it's not serving users it's running inference it's just running inference and then you know throwing
away most of the output because some of the output is good um and because I'm grading it right and it's doing they're doing this while they're also updating
the model in other areas so the the whole Paradigm of training you know pre-training is is is not something down it's just it's logarithmically more
expensive each for each generation for each incremental Improvement people are finding other ways to but there's other
ways to not just continue this but hey I don't need a logarithmic increase in spend to get the next generation of
improvement in fact through this reasoning uh training and inference I can get that logiic Improvement in the
model without ever spending that now I'm going to I'm going to do both right because this is a because each model
jump has unlocked huge value right I me the the you know the thing that I I think so interesting you know I hear
Kramer on CNBC this morning you know and they're talking about is this Cisco from 2000 I was in Omaha Bill Sunday night
for dinner um you know they're obviously big investors and utilities and they're watching what's going on in the data
center buildout and they're like is this Cisco from 2000 so I had my team pull up a chart for Cisco you know 2000 and and
we'll show it on the Pod but you know they peaked at like 120 PE right and yeah you know if you look
at the fallof that occurred in revenue and in IAH you know and then it had 70% compression in the in the priced
earnings multiple right so the priced earnings multiple went from 120 down to something closer to 30 and so I said to
you know in this dinner conversation I said well nvidia's you know PE today is
30 it's not 120 right so you would have to think that there would be 70% PE compression from here or that their
revenue was going to fall by 70% or that their earnings were going to fall by 70% you know in order to have a Cisco like
event we all have post-traumatic stress about that I mean hell I you know I live through that too nobody wants to repeat
that but when people make that comparison it strikes me as uninformed right um it's not to say that there can't be a pullback but given what you
just told us about the buildout next year given what you told us about scaling laws continuing you know um what
do you think when you hear you know the Cisco comparison when people are talking about Nvidia yeah so I I think there's
um a couple things that are not fair right Cisco's Revenue a lot of it was funded through uh private credit
investments into building out Telecom infrastructure right uh when we look at nvidia's Revenue sources very little of
it is private credit right and in some cases yes it's private credit like cor weave right but cor weave is just back stopped by Microsoft there is
significant amounts of like different in like what is the source of the capital right the other thing is at the peak of
the you know especially when to inflation adjust it um the the private
Capital entering the space was much larger than it is today right as much as people say the Venture markets are going
crazy throwing these huge valuations at you know all these companies and we were just talking about this before the show
but like hey the The Venture markets the private markets have not even tapped in right guess what private Market money like in the Middle East and these
Sovereign wealth funds it's not coming in yet has barely come in right why why why why wouldn't there be a lot more
spend from them as well right um and so there is a significant amount of the difference of capital the source is positive cash flows of the most
profitable companies that have ever lived or ever existed in humanity versus credit speculatively spent right so I
think that is like a big aspect uh that also gives it a bit of a a knob right these companies that are profitable will
be a bit more rational I think I think Corporate America is investing more in
Ai and with more conviction than they did even in the internet wave also maybe we can switch a little bit you've
mentioned inference time reasoning a few times now is clearly a new Vector of
scaling intelligence and I read some of your analysis recently about how
inference time reasoning is way more compute intensive right than simply pre-train you know scaling pre-training
why don't you walk us through we have a really interesting graph here about why that's the case um that we'll we'll post as well but why don't you walk us
through first just kind of what inference time reasoning is from the perspective of compute consumption why
it's so much more compute intensive and so leading to the conclusion that if if
this is in fact going to continue to scale as as a new Vector uh of of of of intelligence it it looks like it's going
to be even more computer intensive than what came before it yeah so pre-training maybe slowing out or it's it's too too expensive but there's these other
aspects of synthetic data generation and inference time compute inference time compute is um on the surface sounds
amazing right I don't need to spend more training a model but when you think about it for a second this is actually very very uh this is not the way you
want to scale you only do that because you have to right um the way because right think about it gbd4 was trained
with hundreds of millions of dollars and it's generating billions of dollars of Revenue hundreds of millions of dollars hundreds of millions of dollars to train
GD and it's generating billions of dollars of Revenue So when you say like
hey Microsoft's capex is nuts sure but their spend on gp4 was very reasonable
relative to the ROI they're getting out of it right now when you say hey I want the next gain um if I just spend you
know sort of a large amount of capital and train a better model awesome but if I don't have to spend that large amount of capital and I deploy it you know I
deploy this better model without you know at the time of Revenue generation rather than ahead of time when I'm training the model this also sounds
awesome but this this comes with this big trade-off right um when you're running reasoning right you you're
having the model generate a lot and then the answer is only a portion of that
right today when you open up chat GPT use gbd4 40 you say something you get a response correct you send something you
get a response whatever it is right all of the stuff that's being generated is being sent to you now you're having this reasoning phase right and open AI does
want to show you but you know there's some open source Chinese models like Alibaba and deep seek they've released some open source models which are not quite as good as open ey of course but
they show you what that reasoning looks like if you want to and open has really some examples it generates tons of things it's like it sometimes switches
between Chinese and English right like whatever it is it's thinking right it's turning it's like this this this this oh should I do it this way should I break it down in these steps and then it comes
out with an answer right now on the surface awesome I didn't have to spend any more on R&D or Capital right saying
this in the loose terms they don't they don't C they don't treat training models as R&D I think on Microsoft on a financial basis but uh they don't have
to treat this they don't have this R&D ahead of time right you you get it at spend time but think about what that
means right if for you right for example one simple thing that we you know we've done a lot of tests on is um hey
generate me this uh this soft this this code right like make this function great uh I put I describe the function in you
know a few hundred words I get back a response that's you know Words awesome
and I'm paying per token when I do this with 01 or any other reasoning model I'm sending the same response right few hundred tokens I'm paying for that I'm
getting the same response roughly you know thousand tokens but in the middle there was 10,000 tokens of it thinking right now what what does that 10,000
tokens of thinking actually mean means well the model's spinning out 10 times as many tokens well if Microsoft's generating you know call it $10 billion
of inference revenue and their margins on that are good they've stated this right they're anywhere from 50 to 70% um
depending on how you count the open AI profit share um you know anywhere from 50 to 70% gross margins their cost for
that is a few billion dollars for $10 billion of Revenue right if now obviously the better model gets to
charge more right so 01 does charge a lot more but you're now increasing your cost from hey I outputed a th000 tokens
to I outputed 11,000 tokens I've 10x to my spend to generate now not the same
thing right it's higher quality correct right um and and and that's that's only part of it that's deceptively simple it's not just 10x right because if you
go look at 01 despite it being the same uh model architecture as GPD 40 it actually costs significantly more per token as well and that's because of you
know sort of this chart that we're looking at here right um and this chart shows hey what is GPD 40 right if I'm generating you know call it a thousand
tokens right and and that's what GPD 4 on the bottom right is uh of llama 405b this is open model so it's easier to
simulate um you know the exact uh metrics of it but you know if I'm doing that I'm keeping my users's uh you know
experience of the model constant um I.E the number of tokens they're getting at the speed then you know when I when I
ask it a question it generates the unit it generates the code whatever it is I'm
I can group together many users requests I can group together over 256 users
requests on one server for llama 405b of of of of Nvidia server right like you know $300,000 server or so when I do
this with 01 right because it's doing doing that thinking phase of 10,000 right this is this is basically the whole context length thing context
length is not free right uh context length or sequence length means that it has to calculate attention attention mechanism I.E it spends a lot of memory
on generating this KB cache and reading this KB cache constantly now the maximum
batch size I.E concurrent users I can have is a fraction of that 1/4 to one5
the number of users can currently use this server so not only do I need to generate 10x as many tokens
each each token that's generated is four to 5x less users so the cost increase is
is stupendous when you think about a a single user cost increase for a single token to be generated is four to 5x but then I'm generating 10x as many tokens
so you could argue the cost increase is 50x for an 01 style model on input to Output I knew the 10x CU it was on the
original 01 release but with the log scale but I didn't know well it's 10 and it just requires you to have you know
again to service the same number of customers you have to have multiples more compute well this there's good news
and bad news here Brad which I think is what Dylan's telling us if if you're just selling Nvidia hardware and they
Remain the architecture and this is our scaling path you're going to consume way more of it correct um but the margins
for the people who are generating on the other end unless they can pass it on to the end consumer are going to you know are going to compress and and the thing
is you can pass it on to theend Consumer because hey it's not really like oh oh it's x% better on this Benchmark it's it
literally could not do this before and now it can right um and so and they're running a test right now where they're 10 Xing what they're charging the end
consumer you know uh and and it's 10x per token right corre remember they're also paying for 10x as many tokens right
so it's actually you know the consumer is paying 50x more per query right um but they're getting value out of it
because now all of a sudden it can pass certain benchmarks like sbench right software engineering Benchmark right which is just a benchmark for generating
like you know decent code right um there's front-end web development right what do you pay frontend web developers what do you pay backend uh developers
versus hey what if they use 01 how much more code can they output how much more can they output yes the queries are expensive but they're nothing close to
the human right and so each level of productivity gain I get um each level of capabilities jump is a whole new class
of tasks that it can do right and and therefore I can charge for that right so this is the whole like axes of yes I
spend a lot more to get the same output but you're not you're not getting the same output with this model are we
overestimating or under underestimating end demand Enterprise level demand for
the 01 model what are you hearing so I would say the 01 style model is so early days people don't even like get it right
01 is like they just cracked the code and they're doing it but guess what right now on you know some of the uh
Anonymous benchmarks there you know it's called llm Cy which is like an arena where different llms get to like compete
sort sort of and people vote on them there's a Google model that is doing reasoning right now and it's not released yet but it's going to be
released soon enough right and thropic is going to release a reasoning model these people are going to one up each other and also they've spent so little
compute on reasoning right now in terms of training time and they see a very clear path to spending a lot more I.E
jumping up the scaling laws oh I only spent $10 million well wait that means I can jump up two to three logarithms in scaling like that because I've already
got the compute um you know I can go from 10 to1 million billion to1 billion on reasoning in such a quick succession
and so the this the performance improvements will get out of these models is is humongous right in in the coming you know six months to a year in
certain benchmarks where you have functional verifiers um quick question and I we promised we'd go to these
Alternatives so we'll have to get there eventually but um if you go back we
we've use this internet wave comparison multiple times when all of the venture
companies got started on the internet they were all on Oracle and sun and five years later they weren't on Oracle or
sun and some have argued it went from a development sandbox world to a
optimization world is that likely to happen is there an equivalency here or
not and if you could touch on why the
the the back end is so Steep and Cheap
like you know just you you go a model you know behind or you you like the the
Tok the price you can save by just backing up a little bit is nutty yeah
yeah so um today right like 01 is stupendously expensive you drop down to 40 it's a lot cheaper you jum down to 40
mini it's so cheap why because now I'm comp with 40 mini I'm competing against llama and I'm competing against deep seek I'm competing against mistr I'm
competing against Alibaba and I'm competing against tons of you think those are market clearing prices I I I I
think and in addition right there is also the problem of inferencing a small
model is quite easy right I can run llama 70b on one AMD GPU I can run llama
70b on one Nvidia GPU and soon enough there will be on one set of Amazon's new tranium right I can sort of run this
model on a single chip this is a very easy not I won't say very easy problem still hard it's a quite E bit easier problem than running this complex
reasoning or this very large model right and so there is there is that difference right there's also the fact that hey
there's literally 15 different companies out there offering API inferences INF inference apis on llama and Alibaba and
deeps and mistr like these different models right you're talking about cerebrus and Gro and you know fireworks and all these others yeah fireworks
together um uh you know all the all the companies that aren't using their own Hardware now of course grock and cus are doing their own hardware and doing this
as well but the the market these the margins here are bad right um you know s sort of we had this whole thing about
the inference race to the bottom when mraw relas released their mixw model which was like very revolutionary sort
of late last year um because it was such a level of performance that didn't exist
in the open source um that it drove pricing down so fast right um because everyone's competing for API what am I
as an API provider providing you that like why don't you switch from mine to to to his why because well there's no it's pretty funable right I'm still
getting the same tokens on the same model and so the margins for these guys is much slower so Microsoft's earning 50 to 70% gross margins on open models and
that's with the profit share they get to get or the share that they give the right or you know anthropic similarly in their most recent round they were
showing like 70% gross margins but that's because they have this model you you step down to here no one uses this
model um from you know a lot less people use it from open AI or anthropic because they can just like take the the weights
from met uh llama put it on their own server or vice versa go to one of the many competitive API providers some of them being Venture funded some of them
uh you know and money right so there's all this competition here so not only are you saying I'm taking a step back and it's easier problem I'm and so
therefore like if the model's 10x smaller it's like 15x cheaper to run and on the top on top of that I'm removing that gross margin and so it's it's not
15x cheaper to run it's 30X cheaper to run um and so this is this is sort of the beauty of like well is everything
commodity no but like there is a huge Chase to like if you're deploying in Services that's going to be this is
great for you a uh B you have to have the best model or you're no one if you're one of the labs right and so you see a lot of struggles for the companies
that were trying to build the best models but failing right and arguably not only do you have to have the best model you actually have to have an
Enterprise or a consumer willing to pay for the best model right because at the end of the day you know the the best model implies that somebody's willing to
pay you these high margins right and that's either an Enterprise or a consumer so I think you know you're you're quickly narrowing down to just a
handful of folks who will be able to compete you know in that market on the model side yes I think on the who's
willing to pay for these models is I I think of I think a lot more people will pay for the best model right when we use
models internally right we have we have we have language models go through every regulatory filing and permit to look at data center stuff and pull that out and
tell us where to look and where to not to um and and we just use the best model because it's so cheap right like the data that I'm getting out of it the
value I'm getting out of it is so much higher what model are you using um we're using anthropic actually right now Cloud
3.5 CET new um son it um and so uh just because 01 is a lot better on certain tasks but not necessarily regulatory
filings and permitting and like things like that because the cost of Errors is so much higher right same with a uh a developer right if I can increase a
developer who makes $300,000 a year here in the Bay by 20% that's a lot if I can uh if I can take a team of 100
developers and use 75 or 50 to do the same job or I can ship twice as much code this is so worth using the most
expensive model because 01 as expensive it is relative to to 40 is still super
cheap right the cost for intelligence is so high in society right that's why intelligent jobs are the most you know
high-paying jobs white color jobs right are the most high paying jobs if you can bring down the cost of intelligence or augment intelligence then there's a high
market clearing price for that which is why I think that like sort of the oh yes o1 is expensive um and people will always gravitate to what's the cheapest
thing at a certain level of intelligence but each time we break a new level of intelligence it's not just oh you know we've we got a few more tasks we can do
I think it grows the mode of tasks that can be done dramatically very few people could use gp2 and three right A lot of
people can use gbd4 when we get to that quality of jump that we see for the Next Generation the amount of people that can
use it the tasks that it can do balloons out and therefore the amount of sort of White Collar jobs that it can augment increase productivity on will grow and
therefore the market clearing price for that token will be very super interesting I I could make the other argument that someone that's in a high
volume you know this replacing tons of
customer service calls or whatever um might might be tempted to minimize the
spend absolutely and and and maximize the amount of value add they build around this thing database writes and reads and absolutely so so one of the
funny things I like to uh that the calculations we did is if you take one quarter of Nvidia shipments and you said
all of them are going to inference llama 7B you can give every single person on
earth 100 tokens per minute right um or sorry 100 tokens per second you give
every single person on earth 100 tokens per second which is like absurd yeah um you know so like if we're just deploying
llama 7B quality models we've so overbuilt it's not even funny now if we're deploying things that
can like augment engineers and increase productivity and help us build robotics
or AV or whatever else faster um then that's that's a very different like calculation right and so that's sort of the the whole thing like yes small
models are there but like they're so easy to run and it may just both these things may be true right we're going to have tons of small models running everywhere but the compute cost of them
is so low yeah fair enough bill and I were talking about this earlier with respect to the hard drives um you know that you used to cover but if you look
at the memory Market it's been one of these Boomer bust markets the idea was you would always you know sell these things when they're nearing Peak um you
know you always buy them at trough you don't own them anywhere in you know in between they trade at very low earnings
multiples I'm talking about NX and I'm talking about you know Micron as you think about the shift toward inference
time compute it seems that the memory demanded of these chips um and Jensen
has talked a lot about this just is on a secular shift higher right because um if
they're doing these passes you know and you're running like you said 10 or a 100 or a thousand passes for inference time reasoning you just have to have more and
more memory as this context length expands so uh you know talk to us a little bit about you know kind of how
you think about the memory Market yeah so um you know to sort of like set the stage a little bit more is uh thinking
reasoning models output thousands and thousands of tokens and um when you when when we're looking at Transformers
attention right the the like holy girl of Transformers I.E how how it like understands the entire context uh grows
dramatically and and the KV cache I.E the memory that is keeping track of how
what this like context means is growing quadratically right and therefore if I go from a context length of 10 to 100
it's not just a 10x it's much more right um and so you you treat that right like today's reasoning models they'll think
10,000 tokens 20,000 tokens when we get to hey what is complex reasoning going to look like models are going to get to the point where they're thinking for
hundreds of thousands of tokens and there and then this is all one Chain of Thought or it might be some search but it's going to be thinking a lot and this
KV cach is going to balloon so you're saying you're saying memory could grow faster than GPU and it objectively is

